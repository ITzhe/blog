[{"content":"环境信息 系统版本 Docker LDAP Ubuntu 18 20.10 2.4.57 安装OpenLDAP 1 2 3 4 5 6 7 8 docker run -d \\ -p 389:389 \\ --name openldap \\ --restart=always \\ --env LDAP_ORGANISATION=\u0026#34;caizhe\u0026#34; \\ --env LDAP_DOMAIN=\u0026#34;caizhe.org\u0026#34; \\ --env LDAP_ADMIN_PASSWORD=\u0026#34;111111\u0026#34; \\ osixia/openldap 安装web管理端（PHPLDAPadmin） 1 2 3 4 5 6 7 docker run -d \\ --privileged \\ -p 80:80 \\ --name phpldapadmin \\ --env PHPLDAPADMIN_HTTPS=false \\ --env PHPLDAPADMIN_LDAP_HOSTS=192.168.1.100 \\ osixia/phpldapadmin 测试命令： 1 ldapsearch -x -H ldap:/// -D \u0026#34;cn=admin,dc=caizhe,dc=org\u0026#34; -w 111111 -b \u0026#34;dc=caizhe,dc=org\u0026#34; -LLL 或者在容器里执行\n1 ldapsearch -x -H ldap:/// -D \u0026#34;cn=admin,dc=caizhe,dc=org\u0026#34; -w 111111 -b \u0026#34;dc=caizhe,dc=org\u0026#34; -LLL 创建组 LDAP新建用户\n建立用户之前先建立一个group\n选择default\n这里可以随便选一个单位，后期还要改！\n确认无误提交即可，组算是创建完毕了\n创建用户 LDAP组和人员之前默认是不关联的，我们需要把刚才新建的用户添加到“dba-group”组里\n最终的结果是这样的，“Update~”提交即可\n用户已经加入这个组了，我们来看一下，重点是memberOf，咱们Jenkins对组授权就是通过读取memberOf这个字段来判断的，（很想问问作者为什么不能去读取DN，根据OU来判断）\n密码策略 密码策略是一般是使用ppolicy，可以使用下面命令查看是否加载了ppolicy模块\n1 2 3 4 5 6 7 8 9 10 11 12 root@openldap-host:/etc/ldap# slapcat -n 0 | grep -i module dn: cn=module{0},cn=config objectClass: olcModuleList cn: module{0} olcModulePath: /usr/lib/ldap olcModuleLoad: {0}back_mdb olcModuleLoad: {1}memberof olcModuleLoad: {2}refint structuralObjectClass: olcModuleList olcAttributeTypes: {15}( 1.3.6.1.4.1.4754.1.99.1 NAME \u0026#39;pwdCheckModule\u0026#39; DESC \u0026#39;Loadable module that instantiates \u0026#34;check_password() function\u0026#39; EQUALITY cas op AUXILIARY MAY pwdCheckModule ) 没有的话需要ldapadd命令额外加载模块，新建一个ldif文件\n1 2 3 4 5 cat load-ppolicy-mod.ldif dn: cn=module{0},cn=config changetype: modify add: olcModuleLoad olcModuleLoad: ppolicy.la 加载命令：\n1 ldapadd -Y EXTERNAL -H ldapi:/// -f load-ppolicy-mod.ldif 检查是否加载成功\n1 2 3 4 5 6 7 8 9 10 11 12 13 root@openldap-host:/etc/ldap# slapcat -n 0 | grep -i module dn: cn=module{0},cn=config objectClass: olcModuleList cn: module{0} olcModulePath: /usr/lib/ldap olcModuleLoad: {0}back_mdb olcModuleLoad: {1}memberof olcModuleLoad: {2}refint olcModuleLoad: {3}ppolicy.la structuralObjectClass: olcModuleList olcAttributeTypes: {15}( 1.3.6.1.4.1.4754.1.99.1 NAME \u0026#39;pwdCheckModule\u0026#39; DESC \u0026#39;Loadable module that instantiates \u0026#34;check_password() function\u0026#39; EQUALITY cas op AUXILIARY MAY pwdCheckModule ) 检查现有LDAP策略的数据库存储格式，发现均为mdb格式\n1 2 3 4 5 6 7 8 root@openldap-host:/etc/ldap# ldapsearch -LLL -Y EXTERNAL -H ldapi:/// -b cn=config olcDatabase | grep mdb SASL/EXTERNAL authentication started SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth SASL SSF: 0 dn: olcDatabase={1}mdb,cn=config olcDatabase: {1}mdb dn: olcOverlay={0}memberof,olcDatabase={1}mdb,cn=config dn: olcOverlay={1}refint,olcDatabase={1}mdb,cn=config 创建默认密码策略的DN\n1 2 3 4 5 6 7 root@openldap-host:/etc/ldap# cat pwpolicyoverlay.ldif dn: olcOverlay=ppolicy,olcDatabase={1}mdb,cn=config objectClass: olcOverlayConfig objectClass: olcPPolicyConfig olcOverlay: ppolicy olcPPolicyDefault: cn=default,dc=caizhe,dc=org olcPPolicyHashCleartext: TRUE 更新数据库\n1 2 3 4 5 root@498d5b127557:/# ldapadd -Y EXTERNAL -H ldapi:/// -f pwpolicyoverlay.ldif SASL/EXTERNAL authentication started SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth SASL SSF: 0 adding new entry \u0026#34;olcOverlay=ppolicy,olcDatabase={1}mdb,cn=config\u0026#34; 添加默认策略规则\n这里为了测试（密码过期设置了100秒，60秒前会有提醒）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 cat ldap-pwpolicies.ldif dn: cn=default,dc=caizhe,dc=org objectClass: inetOrgPerson objectClass: pwdPolicyChecker objectClass: pwdPolicy cn: pwpolicy sn: pwpolicy pwdAttribute: userPassword pwdMinAge: 0 pwdMaxAge: 100 pwdInHistory: 5 pwdCheckQuality: 2 pwdMinLength: 8 pwdExpireWarning: 60 pwdGraceAuthNLimit: 3 pwdLockout: TRUE pwdLockoutDuration: 0 pwdMaxFailure: 0 pwdFailureCountInterval: 0 pwdReset: TRUE pwdMustChange: TRUE pwdAllowUserChange: TRUE pwdSafeModify: FALSE 更新 slapd 上的密码策略(创建一个默认的策略，名字叫default)\n1 2 3 4 5 ldapadd -Y EXTERNAL -H ldapi:/// -f ldap-pwpolicies.ldif SASL/EXTERNAL authentication started SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth SASL SSF: 0 adding new entry \u0026#34;cn=default,dc=caizhe,dc=org\u0026#34; 测试 修改密码为不符合要求：\n1 2 3 4 5 6 7 8 root@openldap-host:/etc/ldap# ldappasswd -H ldapi:/// -Y EXTERNAL -S \u0026#34;cn=ccc,cn=default,ou=pwpolicy,dc=caizhe,dc=org\u0026#34; New password: Re-enter new password: SASL/EXTERNAL authentication started SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth SASL SSF: 0 Result: Constraint violation (19) Additional info: Password fails quality checking policy 新建用户登录测试密码有效期：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 64c1e4ea conn=1068 fd=14 ACCEPT from IP=192.168.0.131:50662 (IP=0.0.0.0:389) 64c1e4ea conn=1068 op=0 BIND dn=\u0026#34;cn=aaa,dc=caizhe,dc=org\u0026#34; method=128 64c1e4ea conn=1068 op=0 BIND dn=\u0026#34;cn=aaa,dc=caizhe,dc=org\u0026#34; mech=SIMPLE ssf=0 64c1e4ea ppolicy_bind: Setting warning for password expiry for cn=aaa,dc=caizhe,dc=org = 21 seconds\t#还有21秒过期 64c1e4ea conn=1068 op=0 RESULT tag=97 err=0 text= 64c1e4ea conn=1068 op=1 SRCH base=\u0026#34;\u0026#34; scope=0 deref=3 filter=\u0026#34;(objectClass=)\u0026#34; 64c1e4ea conn=1068 op=1 SRCH attr=subschemaSubentry 64c1e4ea conn=1068 op=1 SEARCH RESULT tag=101 err=0 nentries=1 text= 64c1e4ea conn=1068 op=2 SRCH base=\u0026#34;cn=Subschema\u0026#34; scope=0 deref=3 filter=\u0026#34;(objectClass=subschema)\u0026#34; 64c1e4ea conn=1068 op=2 SRCH attr=createTimestamp modifyTimestamp 64c1e4ea conn=1068 op=2 SEARCH RESULT tag=101 err=0 nentries=1 text= 64c1e4ea conn=1068 op=3 SRCH base=\u0026#34;\u0026#34; scope=0 deref=0 filter=\u0026#34;(objectClass=)\u0026#34; 64c1e4ea conn=1068 op=3 SRCH attr=namingContexts subschemaSubentry supportedLDAPVersion supportedSASLMechanisms supportedExtension supportedControl supportedFeatures vendorName vendorVersion + objectClass 64c1e4ea conn=1068 op=3 SEARCH RESULT tag=101 err=0 nentries=1 text= 64c1e4ea conn=1068 op=4 SRCH base=\u0026#34;\u0026#34; scope=0 deref=0 filter=\u0026#34;(objectClass=)\u0026#34; 64c1e4ea conn=1068 op=4 SRCH attr= 64c1e4ea conn=1068 op=4 SEARCH RESULT tag=101 err=0 nentries=1 text= 64c1e4ea conn=1068 op=5 SRCH base=\u0026#34;cn=Subschema\u0026#34; scope=0 deref=3 filter=\u0026#34;(objectClass=)\u0026#34; 64c1e4ea conn=1068 op=5 SRCH attr=hasSubordinates objectClass 64c1e4ea conn=1068 op=5 SEARCH RESULT tag=101 err=0 nentries=1 text= 64c1e4ea conn=1068 op=6 SRCH base=\u0026#34;dc=caizhe,dc=org\u0026#34; scope=0 deref=3 filter=\u0026#34;(objectClass=)\u0026#34; 64c1e4ea conn=1068 op=6 SRCH attr=hasSubordinates objectClass 64c1e4ea conn=1068 op=6 SEARCH RESULT tag=101 err=32 nentries=0 text= 64c1e4ea conn=1068 op=7 SRCH base=\u0026#34;cn=config\u0026#34; scope=0 deref=3 filter=\u0026#34;(objectClass=*)\u0026#34; 64c1e4ea conn=1068 op=7 SRCH attr=hasSubordinates objectClass 64c1e4ea conn=1068 op=7 SEARCH RESULT tag=101 err=32 nentries=0 text= 注意：原有账户的密码不受影响，只有后面新加的用户会收到密码策略的限制\n自主改密平台 创建一个配置文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 \u0026lt;?php // My SSP configuration $keyphrase = \u0026#34;mysecret\u0026#34;; $debug = true; $use_captcha = false; $ldap_url = \u0026#34;ldap://192.168.1.250:389\u0026#34;; $ldap_binddn = \u0026#34;CN=admin,DC=innovsharing,DC=com\u0026#34;; $ldap_bindpw = \u0026#34;111111\u0026#34;; $ldap_base = \u0026#34;dc=innovsharing,dc=com\u0026#34;; $ldap_filter = \u0026#34;(\u0026amp;(objectClass=inetOrgPerson)(cn={login}))\u0026#34;; $use_sms = false; $use_questions = false; $who_change_password = \u0026#34;user\u0026#34;; $show_extended_error = true; $pwd_show_policy_pos = \u0026#34;above\u0026#34;; $pwd_show_policy = \u0026#34;always\u0026#34;; #$pwd_no_reuse = true; #$ldap_use_exop_passwd = true; #$ldap_use_ppolicy_control = true; #$pwd_min_lower = 1; #$pwd_min_upper = 1; #$pwd_min_digit = 1; $pwd_min_length = 8; #$pwd_max_length = 16; #$hash = \u0026#34;MD5\u0026#34;; ## Token # Use tokens? # true (default) # false $use_tokens = true; # Crypt tokens? # true (default) # false $crypt_tokens = true; # Token lifetime in seconds $token_lifetime = \u0026#34;3600\u0026#34;; ## Mail # LDAP mail attribute $mail_attributes = array( \u0026#34;mail\u0026#34;, \u0026#34;gosaMailAlternateAddress\u0026#34;, \u0026#34;proxyAddresses\u0026#34; ); # Get mail address directly from LDAP (only first mail entry) # and hide mail input field # default = false $mail_address_use_ldap = false; # Who the email should come from $mail_from = \u0026#34;xxx@innovsharing.com\u0026#34;; $mail_from_name = \u0026#34;Self Service Password\u0026#34;; $mail_signature = \u0026#34;\u0026#34;; # Notify users anytime their password is changed $notify_on_change = false; # PHPMailer configuration (see https://github.com/PHPMailer/PHPMailer) $mail_sendmailpath = \u0026#39;/usr/sbin/sendmail\u0026#39;; $mail_protocol = \u0026#39;smtp\u0026#39;; $mail_smtp_debug = 0; $mail_debug_format = \u0026#39;error_log\u0026#39;; $mail_smtp_host = \u0026#39;smtp.exmail.qq.com\u0026#39;; $mail_smtp_auth = true; $mail_smtp_user = \u0026#39;xxxxx@innovsharing.com\u0026#39;; $mail_smtp_pass = \u0026#39;xxxxxxxxxxxxx\u0026#39;; $mail_smtp_port = 465; $mail_smtp_timeout = 30; $mail_smtp_keepalive = false; $mail_smtp_secure = \u0026#39;ssl\u0026#39;; $mail_smtp_autotls = true; $mail_smtp_options = array(); $mail_contenttype = \u0026#39;text/plain\u0026#39;; $mail_wordwrap = 0; $mail_charset = \u0026#39;utf-8\u0026#39;; $mail_priority = 3; ?\u0026gt; 运行新的容器\ndocker run -p 9080:80 --restart=always --name selfServicePassword -v $PWD/config.inc.php:/var/www/conf/config.inc.local.php -itd docker.io/ltbproject/self-service-password:1.5\r直接修改密码即可(如果忘记密码，可以使用邮箱重置密码)\n过期提醒 SSP 可以改密码很方便，但是有一个弊端：\n1、过期之前没有提醒；\n2、无法修改已经过期的密码（AD可以，但openldap一直提示密码错误）\n编写一个脚本，完成以上功能：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 LDAP_HOSTURI=\u0026#34;ldap://192.168.1.250:389\u0026#34; LDAP_ROOTDN=\u0026#34;cn=admin,dc=innovsharing,dc=com\u0026#34; LDAP_ROOTPW=\u0026#34;111111\u0026#34; LDAP_SEARCHBASEDN=\u0026#34;dc=innovsharing,dc=com\u0026#34; LDAP_SEARCHFILTER=\u0026#34;(\u0026amp;(cn=*)(objectClass=inetOrgPerson))\u0026#34; LDAP_SEARCHCOMM=\u0026#34;ldapsearch -x -H ${LDAP_HOSTURI} -D ${LDAP_ROOTDN} -LLL -w ${LDAP_ROOTPW} ${LDAP_SEARCHFILTER}\u0026#34; EXPIRE_DAY=165\t# ldap 设置了180天过期，提前15天（第165天）发出邮件提醒 ${LDAP_SEARCHCOMM} -b ${LDAP_SEARCHBASEDN} dn|awk \u0026#39;{print $2}\u0026#39; \u0026gt; /tmp/1.log for i in `cat /tmp/1.log` do echo $i ModifyTime=`${LDAP_SEARCHCOMM} -b $i +|grep modifyTimestamp|awk \u0026#39;{print $NF}\u0026#39;|cut -c1-8` ModifyTime=$(date -d \u0026#34;${ModifyTime}\u0026#34; +%s) echo ${ModifyTime} NowTime=$(date +%s) diff=$(( (NowTime - ModifyTime) / 86400 )) echo ${diff} if (( ${diff} \u0026gt; ${EXPIRE_DAY )) then echo \u0026#34;exipire\u0026#34; Mail=`${LDAP_SEARCHCOMM} -b $i mail |grep mail|awk \u0026#39;{print $2}\u0026#39;` echo \u0026#34;Hi,Please change your password. It will expire.\u0026#34; | mail -s \u0026#34;Accont expire warning\u0026#34; ${mail} else echo \u0026#34;no exipire\u0026#34; fi done rm -f /tmp/1.log 放到定时任务，每天执行一遍\n需要安装一些基础命令（mail、gawk、ssmtp）\n1 apt install mailutils gawk ssmtp 填写邮箱配置\n1 2 3 4 5 6 7 8 9 vim /etc/ssmtp/ssmtp.conf root=xxx@innovsharing.com mailhub=smtp.exmail.qq.com:465 AuthUser=xxx@innovsharing.com AuthPass=xxxxxxx UseTLS=Yes ------------------------- vim /etc/ssmtp/revaliases root:xxx@innovsharing.com:smtp.exmail.qq.com:465 发送邮件确认一下\n1 echo \u0026#34;nei rong\u0026#34; | mail -s \u0026#34;zhu ti\u0026#34; xxxxxxx@qq.com 参考链接：\nhttps://www.openldap.org/devel/admin/overlays.html https://tutoriels.meddeb.net/openldap-password-policy-managing-users-accounts/\nhttps://kifarunix.com/implement-openldap-password-policies/\nhttps://tylersguides.com/guides/openldap-password-policy-overlay/\n","date":"2023-07-24T14:29:17+08:00","permalink":"https://caizhe.org/p/docker%E5%AE%89%E8%A3%85openldap/","title":"Docker安装OpenLDAP"},{"content":" 操作系统 Ubuntu 18 ES版本 8.10.1 ES 从8.0之间集群通讯改成了必须使用https的方式，需要先生成SSL证书；\n可以先跑一个单机的ES，然后通过单机ES的命令行生成证书文件\n单机ES运行命令：\n1 2 3 4 5 docker run -d \\ --name elasticsearch \\ -e \u0026#34;discovery.type=single-node\u0026#34; -e ELASTIC_PASSWORD=elastic \\ elasticsearch:8.10.1 进入容器，准备生成证书：\nCA证书：\nelasticsearch-certutil ca\t#一路回车默认即可，最后会生成elastic-stack-ca.p12的文件\r私钥证书：\nelasticsearch-certutil cert --ca elastic-stack-ca.p12\t#一路回车默认即可\r将证书文件拷贝出容器之外并删掉\nES配置文件，并将配置文件拷贝到其他节点：\ncluster.name: elasticsearch-cluster\rnode.name: node1\t# 记得修改node名称\rnetwork.host: 0.0.0.0\rhttp.port: 9200\rtransport.port: 9300\rdiscovery.seed_hosts: - 10.1.1.10:9300\r- 10.1.1.20:9300\r- 10.1.1.30:9300\r## Bootstrap the cluster using an initial set of master-eligible nodes:\rcluster.initial_master_nodes: - node1\r- node2\r- node3\r# 是否支持跨域\rhttp.cors.enabled: true\r# 默认为*表示支持所有域名跨域访问，也可以指定域名跨域，或者使用正则表达式匹配。\rhttp.cors.allow-origin: \u0026quot;*\u0026quot;\r# 跨域允许设置的头信息\rhttp.cors.allow-headers: Authorization\r# 是否返回设置的跨域Access-Control-Allow-Credentials头\rhttp.cors.allow-credentials: true\r# 开启x-pack\rxpack.security.enabled: true\r# 开启ssl认证\rxpack.security.transport.ssl.enabled: true\rxpack.security.transport.ssl.verification_mode: certificate\rxpack.security.transport.ssl.client_authentication: required\r# 配置生成的ca证书，这里的路径可以根据\rxpack.security.transport.ssl.keystore.path: certs/elastic-certificates.p12\rxpack.security.transport.ssl.truststore.path: certs/elastic-certificates.p12\rxpack.security.authc.api_key.enabled: true\r启动命令：\n1 2 3 4 5 6 7 8 9 10 11 docker run -d \\ --name elasticsearch \\ --restart=always \\ --network=host \\ -e ES_JAVA_OPTS=\u0026#34;-Xms2g -Xmx2g\u0026#34; \\ -e ELASTIC_PASSWORD=elastic \\ -p 9200:9200 \\ -p 9300:9300 \\ -v /data/config/es/certs/:/usr/share/elasticsearch/config/certs/ \\ -v /data/config/es/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml \\ elasticsearch:8.10.1 测试命令：curl -k \u0026ndash;user elastic:elastic -XGET http://127.0.0.1:9200/_cat/indices?v\n参考链接：https://www.zsjweblog.com/2022/03/09/elasticsearch8-1-0%e9%9b%86%e7%be%a4%e6%90%ad%e5%bb%ba/\n","date":"2023-06-14T14:29:17+08:00","permalink":"https://caizhe.org/p/docker%E5%AE%89%E8%A3%85es/","title":"Docker安装ES"},{"content":"Redis 5.0.7\n创建集群目录\nmkdir /home/redis-cluster\r1 2 3 4 5 6 7 8 9 port ${PORT} protected-mode no cluster-enabled yes cluster-config-file nodes.conf cluster-node-timeout 5000 cluster-announce-ip 192.168.21.28 //自己服务器IP cluster-announce-port ${PORT} cluster-announce-bus-port 1${PORT} appendonly yes 创建专有网络（可选）\ndocker network create redis-net\r创建conf和data目录\n1 2 3 4 5 for port in `seq 7000 7005`; do \\ mkdir -p ./${port}/conf \\ \u0026amp;\u0026amp; PORT=${port} envsubst \u0026lt; ./redis-cluster.tmpl \u0026gt; ./${port}/conf/redis.conf \\ \u0026amp;\u0026amp; mkdir -p ./${port}/data; \\ done 创建Redis容器\n1 2 3 4 5 6 7 for port in `seq 7000 7005`; do \\ docker run -d -ti -p ${port}:${port} -p 1${port}:1${port} \\ -v /home/redis-cluster/${port}/conf/redis.conf:/usr/local/etc/redis/redis.conf \\ -v /home/redis-cluster/${port}/data:/data \\ --restart always --name redis-${port} --net redis-net \\ --sysctl net.core.somaxconn=1024 redis redis-server /usr/local/etc/redis/redis.conf; \\ done 随意进入一个容器\ndocker exec -it e61133e98d01 /bin/bash\r创建集群\nredis-cli --cluster create 192.168.21.28:7000 192.168.21.28:7001 192.168.21.28:7002 192.168.21.28:7002 192.168.21.28:7003 192.168.21.28:7004 192.168.21.28:7005 --cluster-replicas 1\r输出记录\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 \u0026gt;\u0026gt;\u0026gt; Performing hash slots allocation on 7 nodes... Master[0] -\u0026gt; Slots 0 - 5460 Master[1] -\u0026gt; Slots 5461 - 10922 Master[2] -\u0026gt; Slots 10923 - 16383 Adding replica 192.168.21.28:7003 to 192.168.21.28:7000 Adding replica 192.168.21.28:7004 to 192.168.21.28:7001 Adding replica 192.168.21.28:7005 to 192.168.21.28:7002 Adding extra replicas... Adding replica 192.168.21.28:7002 to 192.168.21.28:7000 \u0026gt;\u0026gt;\u0026gt; Trying to optimize slaves allocation for anti-affinity [WARNING] Some slaves are in the same host as their master M: d4b82300dfacedeaf7d50e0c3c06d9b7662603e1 192.168.21.28:7000 slots:[0-5460] (5461 slots) master M: f89d211a74b1e71b4b3aa12ae9491cd0657c9f06 192.168.21.28:7001 slots:[5461-10922] (5462 slots) master M: d02714dc30ed747bc23d8387684d0389b1552a61 192.168.21.28:7002 slots:[10923-16383] (5461 slots) master S: d02714dc30ed747bc23d8387684d0389b1552a61 192.168.21.28:7002 replicates f89d211a74b1e71b4b3aa12ae9491cd0657c9f06 S: 5188757b51c2fc804ab9a09779cc83f3dd08e7e2 192.168.21.28:7003 replicates d4b82300dfacedeaf7d50e0c3c06d9b7662603e1 S: 985e81724ea9b5664a19961f4a758689290856b2 192.168.21.28:7004 replicates d02714dc30ed747bc23d8387684d0389b1552a61 S: 2f12b2eb90938cf2469bd6209f85dc42925e2129 192.168.21.28:7005 replicates d4b82300dfacedeaf7d50e0c3c06d9b7662603e1 Can I set the above configuration? (type \u0026#39;yes\u0026#39; to accept): yes \u0026gt;\u0026gt;\u0026gt; Nodes configuration updated \u0026gt;\u0026gt;\u0026gt; Assign a different config epoch to each node \u0026gt;\u0026gt;\u0026gt; Sending CLUSTER MEET messages to join the cluster Waiting for the cluster to join .... \u0026gt;\u0026gt;\u0026gt; Performing Cluster Check (using node 192.168.21.28:7000) M: d4b82300dfacedeaf7d50e0c3c06d9b7662603e1 192.168.21.28:7000 slots:[0-5460] (5461 slots) master 2 additional replica(s) M: d02714dc30ed747bc23d8387684d0389b1552a61 192.168.21.28:7002 slots:[10923-16383] (5461 slots) master 1 additional replica(s) S: 985e81724ea9b5664a19961f4a758689290856b2 192.168.21.28:7004 slots: (0 slots) slave replicates d02714dc30ed747bc23d8387684d0389b1552a61 S: 2f12b2eb90938cf2469bd6209f85dc42925e2129 192.168.21.28:7005 slots: (0 slots) slave replicates d4b82300dfacedeaf7d50e0c3c06d9b7662603e1 S: 5188757b51c2fc804ab9a09779cc83f3dd08e7e2 192.168.21.28:7003 slots: (0 slots) slave replicates d4b82300dfacedeaf7d50e0c3c06d9b7662603e1 M: f89d211a74b1e71b4b3aa12ae9491cd0657c9f06 192.168.21.28:7001 slots:[5461-10922] (5462 slots) master [OK] All nodes agree about slots configuration. \u0026gt;\u0026gt;\u0026gt; Check for open slots... \u0026gt;\u0026gt;\u0026gt; Check slots coverage... [OK] All 16384 slots covered. 连接测试\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 e61133e98d01 redis \u0026#34;docker-entrypoint.s…\u0026#34; 25 minutes ago Up 25 minutes 0.0.0.0:7005-\u0026gt;7005/tcp, 6379/tcp, 0.0.0.0:17005-\u0026gt;17005/tcp redis-7005 4860f478463f redis \u0026#34;docker-entrypoint.s…\u0026#34; 25 minutes ago Up 25 minutes 0.0.0.0:7004-\u0026gt;7004/tcp, 6379/tcp, 0.0.0.0:17004-\u0026gt;17004/tcp redis-7004 16d6bb61118b redis \u0026#34;docker-entrypoint.s…\u0026#34; 25 minutes ago Up 25 minutes 0.0.0.0:7003-\u0026gt;7003/tcp, 6379/tcp, 0.0.0.0:17003-\u0026gt;17003/tcp redis-7003 7b6070c17724 redis \u0026#34;docker-entrypoint.s…\u0026#34; 25 minutes ago Up 25 minutes 0.0.0.0:7002-\u0026gt;7002/tcp, 6379/tcp, 0.0.0.0:17002-\u0026gt;17002/tcp redis-7002 afd85937ac9a redis \u0026#34;docker-entrypoint.s…\u0026#34; 25 minutes ago Up 25 minutes 0.0.0.0:7001-\u0026gt;7001/tcp, 6379/tcp, 0.0.0.0:17001-\u0026gt;17001/tcp redis-7001 0298db107972 redis \u0026#34;docker-entrypoint.s…\u0026#34; 25 minutes ago Up 25 minutes 0.0.0.0:7000-\u0026gt;7000/tcp, 6379/tcp, 0.0.0.0:17000-\u0026gt;17000/tcp redis-7000 redis-cli -c -h 192.168.21.28 -p 7002 192.168.21.28:7002\u0026gt; CLUSTER NODES 985e81724ea9b5664a19961f4a758689290856b2 192.168.21.28:7004@17004 slave d02714dc30ed747bc23d8387684d0389b1552a61 0 1576224546000 6 connected 2f12b2eb90938cf2469bd6209f85dc42925e2129 192.168.21.28:7005@17005 slave d4b82300dfacedeaf7d50e0c3c06d9b7662603e1 0 1576224547233 1 connected 5188757b51c2fc804ab9a09779cc83f3dd08e7e2 192.168.21.28:7003@17003 slave d4b82300dfacedeaf7d50e0c3c06d9b7662603e1 0 1576224546227 5 connected f89d211a74b1e71b4b3aa12ae9491cd0657c9f06 192.168.21.28:7001@17001 master - 0 1576224546000 2 connected 5461-10922 d02714dc30ed747bc23d8387684d0389b1552a61 192.168.21.28:7002@17002 myself,master - 0 1576224544000 3 connected 10923-16383 d4b82300dfacedeaf7d50e0c3c06d9b7662603e1 192.168.21.28:7000@17000 master - 0 1576224547000 1 connected 0-5460 参考链接：\nhttps://blog.csdn.net/tszxlzc/article/details/86565327\nhttps://www.cnblogs.com/lianggp/articles/8136222.html\n","date":"2023-06-14T14:29:17+08:00","permalink":"https://caizhe.org/p/docker%E5%AE%89%E8%A3%85redis-cluster/","title":"Docker安装Redis Cluster"},{"content":"apiserver调用分为3种：\n使用官方的SDK 在Pod内部使用Token，使用443端口，权限控制的比较好，迁移也方便（推荐） 对6443端口发起请求 官方SDK 官方的GitHub有很多语言的支持，有Go、Java、Perl、Ruby； 栗子也很多也很简单，这里我就不费口舌了，自行Google。\nPython SDK地址：https://github.com/kubernetes-client/python\nPod内部调用 内部先获取Token，然后发起请求对apiserver\n1 2 TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token) curl --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; -s https://10.96.0.1:443/api/v1/namespaces/default/pods/ 参考链接：https://blog.csdn.net/russle/article/details/105333738\n6443 端口请求 Shell 版本：\n1 2 3 4 curl https://192.168.1.100:6443/api/v1/nodes \\ --cacert /etc/kubernetes/pki/ca.crt \\ --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt \\ --key /etc/kubernetes/pki/apiserver-kubelet-client.key Python 版本：\n1 2 3 4 5 6 7 8 url = \u0026#34;https://ip:6443/api/v1/nodes/unis6\u0026#34; try: res = requests.get(url, verify=\u0026#34;/etc/kubernetes/pki/ca.pem\u0026#34;, cert=(\u0026#34;/etc/kubernetes/pki/apiserver-kubelet-client.crt\u0026#34;,\u0026#34;/etc/kubernetes/pki/apiserver-kubelet-client.key\u0026#34;),timeout=15) except Exception as e: print(e) else: print(\u0026#34;ok\u0026#34;) metrics 接口一些调用方法： https://blog.csdn.net/u014106644/article/details/84839055\n","date":"2023-06-13T10:51:32+08:00","permalink":"https://caizhe.org/p/kubernetesapi%E8%B0%83%E7%94%A8%E6%96%B9%E6%B3%95/","title":"KubernetesApi调用方法"},{"content":"主节点：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 [root@59-139 ~]# docker run -d --net=host --cap-add NET_ADMIN \\ -e KEEPALIVED_AUTOCONF=true \\ # 角色 \\ -e KEEPALIVED_STATE=MASTER \\ # 绑定的网卡 \\ -e KEEPALIVED_INTERFACE=ens33 \\ # keep alive 通讯的标识ID \\ -e KEEPALIVED_VIRTUAL_ROUTER_ID=2 \\ # 广播地址 \\ -e KEEPALIVED_UNICAST_SRC_IP=192.168.59.139 \\ -e KEEPALIVED_UNICAST_PEER_0=192.168.59.140 \\ # 绑定的网卡 \\ -e KEEPALIVED_TRACK_INTERFACE_1=ens33 \\ # 虚拟VIP \\ -e KEEPALIVED_VIRTUAL_IPADDRESS_1=\u0026#34;192.168.59.100/24 dev ens33\u0026#34; \\ arcts/keepalived 从节点：\n1 2 3 4 5 6 7 8 9 10 [root@59-139 ~]# docker run -d --net=host --cap-add NET_ADMIN \\ -e KEEPALIVED_AUTOCONF=true \\ -e KEEPALIVED_STATE=BACKUP \\ -e KEEPALIVED_INTERFACE=ens33 \\ -e KEEPALIVED_VIRTUAL_ROUTER_ID=2 \\ -e KEEPALIVED_UNICAST_SRC_IP=192.168.59.140 \\ -e KEEPALIVED_UNICAST_PEER_0=192.168.59.139 \\ -e KEEPALIVED_TRACK_INTERFACE_1=ens33 \\ -e KEEPALIVED_VIRTUAL_IPADDRESS_1=\u0026#34;192.168.59.100/24 dev ens33\u0026#34; \\ arcts/keepalived 参考链接：https://hub.docker.com/r/arcts/keepalived\n","date":"2023-03-13T17:44:11+08:00","permalink":"https://caizhe.org/p/docker%E5%AE%89%E8%A3%85keepalived/","title":"Docker安装Keepalived"},{"content":" 名称 版本号 操作系统 Ubuntu 18 k8s 1.22 kubeVIP 0.5 nginx-ingress 1.3.1 关闭防火墙\nufw disable\r设置主机名\n1 hostnamectl set-hostname master1.innovsharing.com 设置hosts\n1 2 172.20.55.100 api.k8s.local 172.20.55.46 master1.innovsharing.com 开启内核IPV4转发\n1 2 3 4 cat /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 bridge-nf 使得 netfilter 可以对 Linux 网桥上的 IPv4/ARP/IPv6 包过滤。比如，设置net.bridge.bridge-nf-call-iptables＝1后，二层的网桥在转发包时也会被 iptables的 FORWARD 规则所过滤。常用的选项包括：\nnet.bridge.bridge-nf-call-arptables：是否在 arptables 的 FORWARD 中过滤网桥的 ARP 包\nnet.bridge.bridge-nf-call-ip6tables：是否在 ip6tables 链中过滤 IPv6 包\nnet.bridge.bridge-nf-call-iptables：是否在 iptables 链中过滤 IPv4 包\nnet.bridge.bridge-nf-filter-vlan-tagged：是否在 iptables/arptables 中过滤打了 vlan 标签的包。\n开启IPVS支持\n1 2 3 4 5 6 7 8 9 10 11 root@master1:~# cat /etc/modules-load.d/k8s.conf br_netfilter ip_vs ip_vs_rr ip_vs_wrr ip_vs_sh ## use nf_conntrack instead of nf_conntrack_ipv4 for Linux kernel 4.19 and later ## nf_conntrack_ipv4 nf_conntrack systemctl restart systemd-modules-load.service 重启服务器，然后执行lsmod | grep -e ip_vs -e nf_conntrack_ipv4,检查是否开启\n1 2 3 4 5 6 7 ip_vs_sh 16384 0 ip_vs_wrr 16384 0 ip_vs_rr 16384 0 ip_vs 155648 6 ip_vs_rr,ip_vs_sh,ip_vs_wrr nf_conntrack 139264 1 ip_vs nf_defrag_ipv6 24576 2 nf_conntrack,ip_vs libcrc32c 16384 2 nf_conntrack,ip_vs 安装IPVS\n1 apt-get install -y ipset ipvsadm 安装crontained\n1 2 3 4 5 6 7 8 wget https://github.com/containerd/containerd/releases/download/v1.6.8/cri-containerd-cni-1.6.8-linux-amd64.tar.gz # 或者国内下载： # wget https://download.fastgit.org/containerd/containerd/releases/download/v1.5.5/cri-containerd-cni-1.5.5-linux-amd64.tar.gz tar -C / -xzf cri-containerd-cni-1.6.8-linux-amd64.tar.gz mkdir -p /etc/containerd containerd config default \u0026gt; /etc/containerd/config.toml 配置crontained，文件位于/etc/containerd/config.toml\n1 2 3 4 5 6 7 8 9 10 11 12 [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc.options] SystemdCgroup = true [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.mirrors] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.mirrors.\u0026#34;docker.io\u0026#34;] endpoint = [\u0026#34;https://bqr1dr1n.mirror.aliyuncs.com\u0026#34;] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.mirrors.\u0026#34;k8s.gcr.io\u0026#34;] endpoint = [\u0026#34;https://registry.aliyuncs.com/google_containers\u0026#34;] sandbox_image = \u0026#34;registry.aliyuncs.com/k8sxio/pause:3.6\u0026#34; 启动crontained\n1 2 3 4 5 6 7 8 9 10 11 12 13 systemctl daemon-reload systemctl enable containerd --now root@master1:~# ctr version Client: Version: v1.6.8 Revision: 9cd3357b7fd7218e4aec3eae239db1f68a5a6ec6 Go version: go1.17.13 Server: Version: v1.6.8 Revision: 9cd3357b7fd7218e4aec3eae239db1f68a5a6ec6 UUID: 8804ad26-c5c3-4320-846a-b713c2307d5e 安装kube-VIP\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 mkdir -p /etc/kubernetes/manifests/ export VIP=172.20.55.100 export INTERFACE=eth0 ctr run --rm --net-host \\ docker.io/plndr/kube-vip:v0.5.0 \\ vip /kube-vip manifest pod \\ --interface $INTERFACE \\ --vip $VIP \\ --controlplane \\ --arp \\ --leaderElection | tee /etc/kubernetes/manifests/kube-vip.yaml \u0026gt; /etc/kubernetes/manifests/kube-vip.yaml kube-vip.yaml 内容\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 apiVersion: v1 kind: Pod metadata: creationTimestamp: null name: kube-vip namespace: kube-system spec: containers: - args: - manager env: - name: vip_arp value: \u0026#34;true\u0026#34; - name: port value: \u0026#34;6443\u0026#34; - name: vip_interface value: eth0 - name: vip_cidr value: \u0026#34;32\u0026#34; - name: cp_enable value: \u0026#34;true\u0026#34; - name: cp_namespace value: kube-system - name: vip_ddns value: \u0026#34;false\u0026#34; - name: vip_leaderelection value: \u0026#34;true\u0026#34; - name: vip_leaseduration value: \u0026#34;5\u0026#34; - name: vip_renewdeadline value: \u0026#34;3\u0026#34; - name: vip_retryperiod value: \u0026#34;1\u0026#34; - name: vip_address value: 172.20.55.100 - name: prometheus_server value: :2112 image: ghcr.io/kube-vip/kube-vip:v0.5.0 imagePullPolicy: Always name: kube-vip resources: {} securityContext: capabilities: add: - NET_ADMIN - NET_RAW volumeMounts: - mountPath: /etc/kubernetes/admin.conf name: kubeconfig hostAliases: - hostnames: - kubernetes ip: 127.0.0.1 hostNetwork: true volumes: - hostPath: path: /etc/kubernetes/admin.conf name: kubeconfig status: {} 安装kubeadm\n1 2 3 4 5 6 7 8 9 10 11 apt-get update \u0026amp;\u0026amp; apt-get install -y apt-transport-https curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - cat \u0026lt;\u0026lt;EOF \u0026gt;/etc/apt/sources.list.d/kubernetes.list deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main EOF apt-get update # apt-get install -y kubelet kubeadm kubectl apt-get install kubelet=1.22.9-00 kubeadm=1.22.9-00 kubectl=1.22.9-00 systemctl enable kubelet 生成K8S 安装配置清单文件\n1 kubeadm config print init-defaults --component-configs KubeletConfiguration \u0026gt; kubeadm.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 apiVersion: kubeadm.k8s.io/v1beta3 bootstrapTokens: - groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authentication kind: InitConfiguration localAPIEndpoint: advertiseAddress: 172.20.55.46 # 本机IP地址 bindPort: 6443 nodeRegistration: criSocket: /run/containerd/containerd.sock # 使用 containerd的Unix socket 地址 imagePullPolicy: IfNotPresent name: master1 # 节点名称 taints: null --- apiVersion: kubeproxy.config.k8s.io/v1alpha1 kind: KubeProxyConfiguration mode: ipvs # kube-proxy 模式 --- apiServer: timeoutForControlPlane: 4m0s apiVersion: kubeadm.k8s.io/v1beta3 certificatesDir: /etc/kubernetes/pki clusterName: kubernetes controllerManager: {} dns: {} etcd: local: dataDir: /var/lib/etcd imageRepository: registry.aliyuncs.com/google_containers # 阿里云镜像站 kind: ClusterConfiguration kubernetesVersion: 1.22.9 controlPlaneEndpoint: 172.20.55.100:6443 # 设置控制平面Endpoint地址 networking: dnsDomain: cluster.local serviceSubnet: 10.96.0.0/12 scheduler: {} --- apiVersion: kubelet.config.k8s.io/v1beta1 authentication: anonymous: enabled: false webhook: cacheTTL: 0s enabled: true x509: clientCAFile: /etc/kubernetes/pki/ca.crt authorization: mode: Webhook webhook: cacheAuthorizedTTL: 0s cacheUnauthorizedTTL: 0s cgroupDriver: systemd clusterDNS: - 10.96.0.10 clusterDomain: cluster.local cpuManagerReconcilePeriod: 0s evictionPressureTransitionPeriod: 0s fileCheckFrequency: 0s healthzBindAddress: 127.0.0.1 healthzPort: 10248 httpCheckFrequency: 0s imageMinimumGCAge: 0s kind: KubeletConfiguration logging: {} memorySwap: {} nodeStatusReportFrequency: 0s nodeStatusUpdateFrequency: 0s resolvConf: /run/systemd/resolve/resolv.conf rotateCertificates: true runtimeRequestTimeout: 0s shutdownGracePeriod: 0s shutdownGracePeriodCriticalPods: 0s staticPodPath: /etc/kubernetes/manifests streamingConnectionIdleTimeout: 0s syncFrequency: 0s volumeStatsAggPeriod: 0s root@master1:~# 下载镜像\n1 2 3 4 5 6 7 kubeadm config images pull --config kubeadm.yaml # core DNS 会报错，找不到，可以打一个tag ctr -n k8s.io i pull docker.io/coredns/coredns:1.8.4 ctr -n k8s.io i tag docker.io/coredns/coredns:1.8.4 registry.aliyuncs.com/k8sxio/coredns:v1.8.4 kubeadm init --upload-certs --config kubeadm.yaml Nginx-Ingress 下载地址：https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.3.1/deploy/static/provider/cloud/deploy.yaml\n让控制器使用主机网络（Deployment）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 ports: - containerPort: 80 name: http hostPort: 80 # 新增加 protocol: TCP - containerPort: 443 name: https hostPort: 443 # 新增加 protocol: TCP spec: hostNetwork: true # 全部使用主机网络 containers: 修改DNS策略：\n1 dnsPolicy: ClusterFirstWithHostNet 修改国内镜像：\nimage: registry.aliyuncs.com/google_containers/nginx-ingress-controller:v1.3.1\rimage: registry.aliyuncs.com/google_containers/kube-webhook-certgen:v1.3.1\rimage: registry.aliyuncs.com/google_containers/kube-webhook-certgen:v1.3.1\rTomcat测试：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 root@OPS:/opt# cat 02-tomcat.yaml apiVersion: v1 kind: Service metadata: name: tomcat namespace: default spec: selector: app: tomcat release: canary ports: - name: http targetPort: 8080 port: 8080 - name: ajp targetPort: 8009 port: 8009 --- apiVersion: apps/v1 kind: Deployment metadata: name: tomcat-deploy namespace: default spec: replicas: 2 selector: matchLabels: app: tomcat release: canary template: metadata: labels: app: tomcat release: canary spec: containers: - name: tomcat image: tomcat:8.5.34-jre8-alpine imagePullPolicy: IfNotPresent ports: - name: http containerPort: 8080 name: ajp containerPort: 8009 --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-myapp namespace: default annotations: kubernetes.io/ingress.class: \u0026#34;nginx\u0026#34; spec: rules: - host: www.example.com http: paths: - path: / pathType: Prefix backend: service: name: tomcat port: number: 8080 普通的 Service： 会生成servicename.namespace.svc.cluster.local的域名，会解析到 Service 对应的 ClusterIP 上，在 Pod 之间的调用可以简写成servicename.namespace， 如果处于同一个命名空间下面​​，甚至可以只写成 ​​servicename​​ 即可访问\nHeadless Service： 无头服务，就是把 clusterIP 设置为 None 的，会被解析为指定 Pod 的 IP 列表，同样还可以通过podname.servicename.namespace.svc.cluster.local访问到具体的某一个 Pod。\n","date":"2023-03-13T11:51:59+08:00","permalink":"https://caizhe.org/p/kubenertes1.22%E9%AB%98%E5%8F%AF%E7%94%A8%E5%AE%89%E8%A3%85/","title":"Kubenertes1.22高可用安装"},{"content":"镜像 拉取镜像\nctr image pull docker.io/library/nginx:latest\r查看镜像\nctr image ls -q\rTAG标签\nctr image tag docker.io/library/nginx:latest caizhe.org/library/nginx:latest\r删除镜像\nctr images rm caizhe.org/library/nginx:latest\r挂载镜像到本地目录\nctr image mount docker.io/library/nginx:latest /mnt\r取消挂载\nctr image unmount /mnt\r导出\nctr image export nginx.tar.gz docker.io/library/nginx:latest\r导入\nctr i import nginx.tar.gz\r解决ctr: content digest sha256:xxxxxx not found的错误\n1 2 3 4 5 6 7 8 方法1： ctr image pull --platform amd64 docker.io/library/nginx:latest ctr i export --platform amd64 nginx.tar.gz docker.io/library/nginx:latest ctr i import --platform amd64 nginx.tar.gz 方法2： ctr i pull --all-platforms docker.io/library/nginx:latest ctr i export --all-platforms nginx.tar.gz docker.io/library/nginx:latest ctr i import nginx.tar.gz 容器 创建容器\nctr container create docker.io/library/nginx:latest nginx\r查看容器\nctr container ls\r删除容器\nctr container rm nginx\r启动容器\nctr task start -d nginx\r直接启动容器（创建容器+启动容器）\nctr run -d --net-host nginx bash\r查看容器状态\nctr task ls\r进入容器\nctr task exec -t --exec-id 0 nginx /bin/bash\r停止容器\nctr task kill nginx\r暂停容器\nctr task pause nginx\r查看容器资源使用情况\nctr task metrics nginx\r名称空间 查看名称空间\nctr ns ls\r创建名称空间\nctr ns create test\r删除名称空间\nctr ns rm test\r提示：docker的名称空间是moby，k8s的名称空间是k8s.io\n","date":"2023-03-02T17:44:11+08:00","permalink":"https://caizhe.org/p/containerd%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","title":"Containerd常用命令"},{"content":"1、下载依赖安装包\n1 2 3 wget https://github.com/containerd/containerd/releases/download/v1.6.4/cri-containerd-cni-1.6.4-linux-amd64.tar.gz # 如果无法访问github可以使用下面地址 # wget https://download.fastgit.org/containerd/containerd/releases/download/v1.6.4/cri-containerd-cni-1.6.4-linux-amd64.tar.gz 2、解压\n1 tar -C / -xzf cri-containerd-cni-1.6.4-linux-amd64.tar.gz 3、添加配置文件：\n默认配置文件为 /etc/containerd/config.toml，我们可以通过如下所示的命令生成\n1 2 mkdir -p /etc/containerd containerd config default \u0026gt; /etc/containerd/config.toml 4、启动containerd\n1 systemctl enable containerd --now 5、验证containerd\n1 2 3 4 5 6 7 8 9 10 # ctr version Client: Version: v1.6.4 Revision: 72cec4be58a9eb6b2910f5d10f1c01ca47d231c0 Go version: go1.16.6 Server: Version: v1.6.4 Revision: 72cec4be58a9eb6b2910f5d10f1c01ca47d231c0 UUID: 38613830-5cd0-4bc4-81b4-2bcdced721d3 ","date":"2023-03-01T17:44:11+08:00","permalink":"https://caizhe.org/p/containerd%E5%AE%89%E8%A3%85/","title":"Containerd安装"},{"content":"1、首先需要关闭远程仓库的分支保护，允许强制推送：\n\u0026ldquo;Settings\u0026rdquo; -\u0026gt; \u0026ldquo;Repository\u0026rdquo; -\u0026gt; scroll down to \u0026ldquo;Protected branches\u0026rdquo;.\n2、克隆项目,拉取所有分支：\n1 2 3 4 5 git clone xxx # 默认拉取master分支 cd xx #进入拉取的文件夹 git branch -r | grep -v \u0026#39;\\-\u0026gt;\u0026#39; | while read remote; do git branch --track \u0026#34;${remote#origin/}\u0026#34; \u0026#34;$remote\u0026#34;; done git fetch --all git pull --all 3、查找大文件（将最大的10个文件查询下来）\n1 git rev-list --objects --all | grep \u0026#34;$(git verify-pack -v .git/objects/pack/*.idx | sort -k 3 -n | tail -10 | awk \u0026#39;{print$1}\u0026#39;)\u0026#34; 4、清理查找的大文件（一次只能处理一个文件/文件夹）：\n1 git filter-branch --force --index-filter \u0026#39;git rm -rf --cached --ignore-unmatch 目录/文件\u0026#39; --prune-empty --tag-name-filter cat -- --all 5、删除并回收空间\n1 2 3 4 5 git for-each-ref --format=\u0026#39;delete %(refname)\u0026#39; refs/original | git update-ref --stdin rm -rf .git/refs/original/ git reflog expire --expire=now --all git gc --prune=now git gc --aggressive --prune=now 6、推送远程仓库：\n1 2 git push origin --force --all git remote prune origin 然后其他人重新克隆项目即可\n参考链接：https://www.msnao.com/2021/06/15/5031.html\n","date":"2023-03-01T17:44:11+08:00","permalink":"https://caizhe.org/p/gitlab%E5%A4%A7%E6%96%87%E4%BB%B6%E6%B8%85%E7%90%86/","title":"Gitlab大文件清理"},{"content":"#volume卷\n分布式卷（distributed）文件通过hash算法随机分布到bricks组成的卷上\n复制式卷（replicated）类似raid1，replica数必须等于volume中brick所包含的存储服务器数。\n条带式卷（striped）类似raid0，strips数必须等于volume中brick所包含的存储服务器数，文件被分布成数据块，以round robin的方式存在bricks中，并发粒度是数据块，大文件性能好。\n分布式条带卷（distributed striped）volume中brick所包含的存储服务器数必须是stripe的倍数（\u0026gt;=2倍），兼顾分布式和条带式的功能。\n分布式复制卷（distributed replicated）volume中brick所包含的存储服务器数必须是stripe的倍数（\u0026gt;=2倍），兼顾分布式和复制卷的功能。\n#分布式卷\n加入存储池\n[root@mystorage1 ~]# gluster peer probe mystorage2\rpeer probe: success. [root@mystorage1 ~]# gluster peer probe mystorage3\rpeer probe: success. [root@mystorage1 ~]# gluster peer probe mystorage4\rpeer probe: success. 在其他节点检查\n[root@mystorage4 ~]# gluster peer status\rNumber of Peers: 3\rHostname: mystorage1\rUuid: 429d3353-dbcc-47d7-b3b5-4c3267fa26df\rState: Peer in Cluster (Connected)\rHostname: mystorage3\rUuid: 7c14cd1f-e801-48ca-8f23-09b017537400\rState: Peer in Cluster (Connected)\rHostname: mystorage2\rUuid: 14b4575a-2ba3-496b-96ea-9a4fe60819b3\rState: Peer in Cluster (Connected)\r初始化\nyum install xfsprogs -y\rmkfs.xfs -f /dev/sdb\rmkdir -p /storage/brick1\rmount /dev/sdb /storage/brick1/\recho \u0026quot;/dev/sdb /storage/brick1/ xfs defaults 0 0\u0026quot; \u0026gt;\u0026gt; /etc/fstab mount -a\r创建卷（整合磁盘）\n[root@mystorage1 ~]# gluster volume create gv1 mystorage1:/storage/brick1/ mystorage2:/storage/brick1/ force volume create: gv1: success: please start the volume to access data\r启动卷\n[root@mystorage1 ~]# gluster volume start gv1\rvolume start: gv1: success\r[root@mystorage4 ~]# gluster volume gv1 info\r挂载卷\n[root@mystorage3 ~]# mount -t glusterfs 127.0.0.1:/gv1 /mnt/\r[root@mystorage3 ~]# df -h\r…………\r127.0.0.1:/gv1 20G 65M 20G 1% /mnt\r把刚才1和2的卷整合在一起之后，共享出来，然后3挂在到gv1种\n测试\n[root@mystorage4 ~]# mount -t glusterfs 127.0.0.1:/gv1 /mnt/\r[root@mystorage4 ~]# mv d4 /mnt/\r[root@mystorage1 ~]# mount -t glusterfs 127.0.0.1:/gv1 /mnt/\r[root@mystorage1 ~]# ls /mnt/\rd4\r#分布式复制卷\n[root@mystorage2 ~]# gluster volume create gv2 replica 2 mystorage3:/storage/brick1/ mystorage4:/storage/brick1/ force volume create: gv2: success: please start the volume to access data\r[root@mystorage2 ~]# gluster volume info gv2\rVolume Name: gv2\rType: Replicate\rVolume ID: 42949fc2-4400-4637-84c7-28aafed7676e\rStatus: Created\rNumber of Bricks: 1 x 2 = 2\rTransport-type: tcp\rBricks:\rBrick1: mystorage3:/storage/brick1\rBrick2: mystorage4:/storage/brick1\rOptions Reconfigured:\rperformance.readdir-ahead: on\r[root@mystorage1 ~]# gluster volume restart gv2\rvolume start: gv2: success\r[root@mystorage1 ~]# df -h\r…… ………\r127.0.0.1:/gv1 20G 65M 20G 1% /mnt\r127.0.0.1:/gv2 10G 33M 10G 1% /opt\r总结：分布巻就像RAID0一样，是一份数据写在了两个地方，把一个数据打散了；而复制卷就像RAID1一样，把一份数据写在了多份。\nmkfs.xfs -f /dev/sdc\rmkdir -p /storage/brick2\rmount /dev/sdc /storage/brick2\recho \u0026quot;/dev/sdc /storage/brick2/ xfs defaults 0 0\u0026quot; \u0026gt;\u0026gt; /etc/fstab df -h\r#gluster volume create gv3 stripe 2 mystorage3:/storage/brick2/ mystorage4:/storage/brick2 force\rgluster volume start gv3\rmkdir /gv1 /gv2 /gv3\r[root@mystorage4 ~]# mount -t glusterfs 127.0.0.1:gv1 /gv1\r[root@mystorage4 ~]# mount -t glusterfs 127.0.0.1:gv2 /gv2\r[root@mystorage4 ~]# mount -t glusterfs 127.0.0.1:gv3 /gv3\r[root@mystorage4 ~]# dd if=/dev/zero bs=1024 count=10000 of=/gv3/10M.file\r[root@mystorage4 ~]# dd if=/dev/zero bs=1024 count=20000 of=/gv3/20M.file\r[root@mystorage4 gv3]# ll -h\rtotal 30M\r-rw-r--r--. 1 root root 9.8M Dec 27 12:19 10M.file\r-rw-r--r--. 1 root root 20M Dec 27 12:19 20M.file\r[root@mystorage4 gv3]# ll /storage/brick2/ -h\rtotal 15M\r-rw-r--r--. 2 root root 4.9M Dec 27 12:19 10M.file\r-rw-r--r--. 2 root root 9.8M Dec 27 12:19 20M.file\r注意看大小\n[root@mystorage3 ~]# mkdir /gv1 /gv2 /gv3\r[root@mystorage3 ~]# mount -t glusterfs 127.0.0.1:gv3 /gv3\r[root@mystorage3 ~]# ll /gv3/\rtotal 30000\r-rw-r--r--. 1 root root 10240000 Dec 27 12:19 10M.file\r-rw-r--r--. 1 root root 20480000 Dec 27 12:19 20M.file\r[root@mystorage3 ~]# ll /storage/brick2/\rtotal 15032\r-rw-r--r--. 2 root root 5128192 Dec 27 12:19 10M.file\r-rw-r--r--. 2 root root 10256384 Dec 27 12:19 20M.file\r注意看大小\n#增加/删除磁盘\n[root@mystorage3 ~]# gluster volume stop gv2\rStopping volume will make its data inaccessible. Do you want to continue? (y/n) y\rvolume stop: gv2: success\r[root@mystorage3 ~]# gluster volume add-brick gv2 replica 2 mystorage1:/storage/brick2/ mystorage2:/storage/brick2 force\rvolume add-brick: success\r[root@mystorage3 ~]# gluster volume start gv2\rvolume start: gv2: success\r[root@mystorage4 gv3]# umount /gv2\r[root@mystorage4 gv3]# mount -t glusterfs 127.0.0.1:gv2 /gv2\r扩容之后还需要磁盘平衡，否则是无法再新加入的磁盘中写入数据的\n[root@mystorage4 gv2]# gluster volume rebalance gv2 start\t[root@mystorage4 gv2]# gluster volume rebalance gv2 status\r注意：增加的brick必须是存储节点的倍数，例如reolica为2，你增加的数量必须是2 4 6 8 ……\ngluster volume stop gv2\rgluster volume remove-brick gv2 replica 2 mystorage1:/storage/brick2/ mystorage2:/storage/brick2 force\r#删除卷 umoun gv1 gluster volume stop gv1 gluster volume delete gv1\n","date":"2019-09-13T10:59:21+08:00","permalink":"https://caizhe.org/p/glusterfs%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/","title":"Glusterfs分布式存储"},{"content":"加斜线分为两种：\n1、localtion 规则后面加/\n2、proxy_pass后面加/\n测试的URL:https://www.xxxx.com/minio/ucenter/imgs/885058fa-dff6-4d60-875d-bb7dd2098ce9.png\nlocaltion 第一种情况：\n1 2 3 location /minio/ { proxy_pass http://minio/; } 结论：加/或者不加/没有区别。\n第二种情况1：\n1 2 3 location /minio/ { proxy_pass http://minio/; } 实际访问的后端地址：127.0.0.1:9000/ucenter/imgs/885058fa-dff6-4d60-875d-bb7dd2098ce9.png\n第二种情况2：\n1 2 3 location /minio/ { proxy_pass http://minio; } 实际访问的后端地址：127.0.0.1:9000/minio/ucenter/imgs/885058fa-dff6-4d60-875d-bb7dd2098ce9.png\n结论：proxy_pass 后面加了斜线，会把location部分的路径去掉，反之不加斜线，会保留location部分的路径\n","date":"2019-03-02T17:44:11+08:00","permalink":"https://caizhe.org/p/nginx-%E5%8A%A0%E6%96%9C%E7%BA%BF%E7%9A%84%E5%8C%BA%E5%88%AB/","title":"Nginx 加斜线的区别"},{"content":"安装依赖：\nyum install openssl-devel -y yum groupinstall \u0026quot;Development Tools\u0026quot; -y\r数据库安装：\nyum install mariadb-server mariadb-devel -y\rsystemctl start mariadb\r删掉一些空用户，否则会报错\ndelete from mysql.user where user='';\rcreate database bind;\r安装bind\ntar xf bind-9.11.5.tar.gz\rcd bind-9.11.5/\r./configure --with-dlz-mysql --enable-largefile --enable-threads=no --prefix=/usr/local/bind --with-openssl # 我试了开线程，但是会报错，所以还是单进程，对DNS的查询操作都是在从库，主库只负责写入和修改操作然后同步到从库，从库直接使用dns的配置文件，所以效率并不影响\rmake -j 4\rmake install\rgroupadd -r named\ruseradd -s /sbin/nologin -M -r -g named named\rchown -R named:named /usr/local/bind echo \u0026quot;export PATH=${PATH}:/usr/local/bind/sbin/:/usr/local/bind/bin/\u0026quot; \u0026gt;\u0026gt; /etc/profile\rsource /etc/profile\r配置bind\ncd /usr/local/bind/etc/\rrndc-confgen -r /dev/urandom \u0026gt;rndc.conf #如果报错直接换成 rndc-confgen \u0026gt;rndc.conf\rmkdir /var/named/\rwget http://www.internic.net/domain/named.root # 下载定义域服务节点\rchown -R named:named /var/named/\r创建配置文件 named.conf\n[root@linux-node3 etc]# cat named.conf options {\rdirectory \u0026quot;/var/named/\u0026quot;;\rrecursion yes;\rlisten-on port 53 { any; };\rdump-file \u0026quot;/var/named/data/cache_dump.db\u0026quot;;\rstatistics-file \u0026quot;/var/named/data/named_stats.txt\u0026quot;;\rallow-query { any; };\rblackhole { none; };\r};\r启动并测试 named -u named -n 1 -c /usr/local/bind/etc/named.conf\rnetstat -lntup|grep 53\rdig www.baidu.com @127.0.0.1\r配置DLZ create database bind;\r建表\nCREATE TABLE IF NOT EXISTS `dns_records` (\r`id` int(10) unsigned NOT NULL AUTO_INCREMENT,\r`zone` varchar(255) NOT NULL,\r`host` varchar(255) NOT NULL DEFAULT '@',\r`type` enum('A','MX','CNAME','NS','SOA','PTR','TXT','AAAA','SVR','URL') NOT NULL,\r`data` varchar(255) DEFAULT NULL,\r`ttl` int(11) NOT NULL DEFAULT '3600',\r`mx_priority` int(11) DEFAULT NULL,\r`view` enum('any', 'Telecom', 'Unicom', 'CMCC', 'ours') NOT NULL DEFAULT \u0026quot;any\u0026quot; ,\r`priority` tinyint UNSIGNED NOT NULL DEFAULT '255',\r`refresh` int(11) NOT NULL DEFAULT '28800',\r`retry` int(11) NOT NULL DEFAULT '14400',\r`expire` int(11) NOT NULL DEFAULT '86400',\r`minimum` int(11) NOT NULL DEFAULT '86400',\r`serial` bigint(20) NOT NULL DEFAULT '2015050917',\r`resp_person` varchar(64) NOT NULL DEFAULT 'ddns.net',\r`primary_ns` varchar(64) NOT NULL DEFAULT 'ns.ddns.net.',\rPRIMARY KEY (`id`),\rKEY `type` (`type`),\rKEY `host` (`host`),\rKEY `zone` (`zone`)\r) ENGINE=MyISAM DEFAULT CHARSET=utf8 AUTO_INCREMENT=1 ;\rCREATE TABLE `dns_xfr` (\r`id` bigint(20) unsigned NOT NULL AUTO_INCREMENT,\r`zone` varchar(255) NOT NULL,\r`client` varchar(255) NOT NULL,\rPRIMARY KEY (`id`),\rKEY `zone` (`zone`),\rKEY `client` (`client`)\r) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8;\r创建用户\ngrant all privileges on bind.* to named@'%' identified by \u0026quot;named\u0026quot;;\rflush privileges;\rnamed.conf 配置DLZ\ndlz \u0026quot;myzone\u0026quot; {\rdatabase \u0026quot;mysql\r{host=192.168.56.11 port=3306 ssl=false dbname=bind user=named pass=named}\r{SELECT zone FROM dns_records WHERE zone = '$zone$' LIMIT 1}\r{SELECT ttl, type, mx_priority, IF(type = 'TXT', CONCAT('\\\u0026quot;',data,'\\\u0026quot;'), data) AS data FROM dns_records WHERE zone = '$zone$' AND host = '$record$' AND type \u0026lt;\u0026gt; '\rSOA' AND type \u0026lt;\u0026gt; 'NS'}\r{SELECT ttl, type, data, primary_ns, resp_contact, serial, refresh, retry, expire, minimum FROM dns_records WHERE zone = '$zone$' AND (type = 'SOA' OR type='NS')\r}\r{SELECT ttl, type, host, mx_priority, IF(type = 'TXT', CONCAT('\\\u0026quot;',data,'\\\u0026quot;'), data) AS data, resp_contact, serial, refresh, retry, expire, minimum FROM dns_reco\rrds WHERE zone = '$zone$' AND type \u0026lt;\u0026gt; 'SOA'}\r{SELECT zone FROM dns_xfr WHERE ( zone='$zone$' OR zone='*' ) AND client = '$client$' LIMIT 1}\u0026quot;;\r};\r插入数据：\ndns_records：\ninsert into bind.dns_records (zone, host, type, data, ttl) VALUES ('test.info', 'www', 'A', '1.1.1.1', '60');\rinsert into bind.dns_records (zone, host, type, data, ttl) VALUES ('test.info', 'bbs', 'A', '1.1.1.1', '60');\rdns_xfr：\n1\t*\t10.31.63.30\r2\t*\t10.31.63.30\r3\t*\t192.168.56.12\r重启bind服务并重新测试\nnamed -u named -n 1 -c /usr/local/bind/etc/named.conf netstat -lntup|grep 53\rdig @127.0.0.1\rdig www.test.info @127.0.0.1\rdig bbs.test.info @127.0.0.1\r从库 named.conf\noptions {\rdirectory \u0026quot;/usr/local/bind/etc/\u0026quot;;\rrecursion yes;\rlisten-on port 53 { any; };\rdump-file \u0026quot;/var/named/data/cache_dump.db\u0026quot;;\rstatistics-file \u0026quot;/var/named/data/named_stats.txt\u0026quot;;\rallow-query { any; };\r};\rkey \u0026quot;rndc-key\u0026quot; {\ralgorithm hmac-md5;\rsecret \u0026quot;J+jRGi+v0DvKOoUoY6pWhQ==\u0026quot;;\r};\rcontrols {\rinet 127.0.0.1 port 953\rallow { any; } keys { \u0026quot;rndc-key\u0026quot;; };\r};\rinclude \u0026quot;/usr/local/bind/etc/view.conf\u0026quot;;\rview.conf\nview \u0026quot;View\u0026quot; {\rzone \u0026quot;.\u0026quot; IN {\rtype hint;\rfile \u0026quot;named.root\u0026quot;;\r};\rzone \u0026quot;u.com\u0026quot; IN {\rtype slave;\rmasters { 192.168.56.11; };\rfile \u0026quot;slaves/u.com.zone\u0026quot;;\r};\r};\r附带一份企业级主库配置文件\noptions {\rlisten-on port 53 { any; };\rlisten-on-v6 port 53 { none; };\rdirectory \u0026quot;/usr/local/bind-dlz/var/named\u0026quot;;\rdump-file \u0026quot;/usr/local/bind-dlz/var/named/data/cache_dump.db\u0026quot;;\rstatistics-file \u0026quot;/usr/local/bind-dlz/var/named/data/named_stats.txt\u0026quot;;\rmemstatistics-file \u0026quot;/usr/local/bind-dlz/var/named/data/named_mem_stats.txt\u0026quot;;\r//max-cache-size 1024m;\rzone-statistics yes;\rtcp-clients 80000;\rstacksize 1000m;\rclients-per-query 100000;\rmax-clients-per-query 100000;\rrecursion yes;\rversion \u0026quot;1.0.0\u0026quot;;\rdnssec-enable no;\rdnssec-validation no;\rbindkeys-file \u0026quot;/etc/bind-dlz/named.iscdlv.key\u0026quot;;\rmanaged-keys-directory \u0026quot;/usr/local/bind-dlz/var/named/dynamic\u0026quot;;\rpid-file \u0026quot;/usr/local/bind-dlz/var/run/named.pid\u0026quot;;\rsession-keyfile \u0026quot;/usr/local/bind-dlz/var/run/session.key\u0026quot;;\rmasterfile-format text;\rserial-query-rate 1000;\rtransfers-out 20000;\rempty-zones-enable no;\rrequest-ixfr no;\rallow-update { none; };\rallow-update-forwarding { none; };\rallow-query { any; };\rallow-query-cache { any ;};\rallow-transfer { slave_list; };\rallow-recursion { any; };\r#allow-recursion { any; };\rnotify yes;\rallow-notify { slave_list; };\rforwarders { 114.114.114.114;119.29.29.29; };\r};\rzone \u0026quot;.\u0026quot; IN {\rtype hint;\rfile \u0026quot;named.ca\u0026quot;;\r};\rinclude \u0026quot;/etc/bind-dlz/named.rfc1912.zones\u0026quot;;\rinclude \u0026quot;/etc/bind-dlz/named.root.key\u0026quot;;\rinclude \u0026quot;/etc/bind-dlz/rndc.key\u0026quot;;\rkey \u0026quot;TSIG-key\u0026quot; {\ralgorithm hmac-md5;\rsecret \u0026quot;J+jRGi+v0DvKOoUoY6pWhQ==\u0026quot;;\r};\rcontrols {\rinet 127.0.0.1 port 953\rallow { any; } keys { \u0026quot;rndc-key\u0026quot;; };\r};\rstatistics-channels {\rinet 127.0.0.1 port 8653 allow { 127.0.0.1; };\r};\r################### log ########################\rlogging {\rchannel default_debug {\rfile \u0026quot;data/named.run\u0026quot;;\rseverity dynamic;\r};\r};\r################### acl ########################\racl \u0026quot;lan_list\u0026quot; {\r172.16.0.0/12;\r192.168.0.0/16;\r10.0.0.0/8;\r127.0.0.1;\r};\racl \u0026quot;slave_list\u0026quot; {\r172.16.1.1;\r172.16.1.252;\r10.10.20.21;\r10.10.20.22;\r192.168.56.12;\r};\r################### zone ########################\rdlz \u0026quot;myzone\u0026quot; {\rdatabase \u0026quot;mysql\r{host=192.168.56.11 port=3306 ssl=false dbname=bind user=named pass=named}\r{SELECT zone FROM dns_records WHERE zone = '$zone$' LIMIT 1}\r{SELECT ttl, type, mx_priority, IF(type = 'TXT', CONCAT('\\\u0026quot;',data,'\\\u0026quot;'), data) AS data FROM dns_records WHERE zone = '$zone$' AND host = '$record$' AND type \u0026lt;\u0026gt; 'SOA' AND type \u0026lt;\u0026gt; 'NS'}\r{SELECT ttl, type, data, primary_ns, resp_contact, serial, refresh, retry, expire, minimum FROM dns_records WHERE zone = '$zone$' AND (type = 'SOA' OR type='NS')}\r{SELECT ttl, type, host, mx_priority, IF(type = 'TXT', CONCAT('\\\u0026quot;',data,'\\\u0026quot;'), data) AS data, resp_contact, serial, refresh, retry, expire, minimum FROM dns_records WHERE zone = '$zone$' AND type \u0026lt;\u0026gt; 'SOA'}\r{SELECT zone FROM dns_xfr WHERE ( zone='$zone$' OR zone='*' ) AND client = '$client$' LIMIT 1}\u0026quot;;\r}; ","date":"2018-08-02T17:44:11+08:00","permalink":"https://caizhe.org/p/bind-dlz/","title":"Bind-DLZ"},{"content":"最近研究安全方向，发现Linux下的一些杀毒软件比较少，今天给大家介绍一款开源的轻量级杀毒软件clamAV。\n特点\n1）GNU开源软件\n2）快速扫描\n3）可以检测35000种病毒，蠕早，特洛依，包括Microsoft Office文档及宏病毒\n4）能够检测压缩文件（Zip RAR Tar Gzip Bzip2……）\n5）强大的邮件扫描功能\n6）扩展性强\n1.时间同步\nntpdate time1.aliyun.com\r2.安装clamAV。（可以使用阿里云的epel源）\nyum install clamav\r3.更新病毒库。（第一次下载可能会花点儿时间）\nfreshclam\r4.测试：下载带病毒软件\nwget http://www.eicar.org/download/eicar_com.zip\r4.使用 clamscan 开始扫描\nclamscan -r /etc/\r…………\r./.rnd: OK\r/eicar_com.zip: Eicar-Test-Signature FOUND\r./.cshrc: OK\r…………\r----------- SCAN SUMMARY -----------\rKnown viruses: 5748467\rEngine version: 0.99.2\rScanned directories: 1\rScanned files: 15\rInfected files: 1\rData scanned: 0.10 MB\rData read: 91.95 MB (ratio 0.00:1)\rTime: 19.179 sec (0 m 19 s)\r可以加上 \u0026ndash;remove 参数，扫描完之后直接杀掉病毒\n可以加上定时任务每天定时扫描和更新病毒库,此处不再介绍\n如果你发现了ClamVA查杀不了的病毒，可以在以下的网址提交相关的信息\nhttp://www.clamav.net/sendvirus.html 官方网址：\nhttp://www.clamav.net/lang/en/\rhttp://www.clamav.net/sendvirus.html ","date":"2018-03-13T10:59:21+08:00","permalink":"https://caizhe.org/p/clamv/","title":"Clamv"},{"content":"最近闲着无聊复习了一下高可用Keepalive和LVS反向代理，觉的挺经典的，整理了一篇文档\n在部署之前，我们先对LVS的一些基础知识了解一下\n#LVS的模式\n###NAT模式\nNAT用法本来是因为网络IP地址不足而把内部保留IP地址通过映射转换成公网地址的一种上网方式(原地址NAT)｡如果把NAT的过程稍微变化,就可以成为负载均衡的一种方式｡原理其实就是把从客户端发来的IP包的IP头目的地址在DR上换成其中一台REALSERVER的IP地址并发至此REALSERVER,而REALSERVER则在处理完成后把数据经过DR主机发回给客户端,DR在这个时候再把数据包的原IP地址改为DR接口上的IP地址即可｡期间,无论是进来的流量,还是出去的流量,都必须经过DR｡\n###FULLNAT 模式\n###TUNNEL模式\n隧道模式则类似于VPN的方式,使用网络分层的原理,在从客户端发来的数据包的基础上,封装一个新的IP头标记(不完整的IP头,只有目的IP部)发给REALSERVER,REALSERVER收到后,先把DR发过来的数据包的头给解开,还原其数据包原样,处理后,直接返回给客户端,而不需要再经过DR｡需要注意的是,由于REALSERVER需要对DR发过来的数据包进行还原,也就是说必须支持IPTUNNEL协议｡所以,在REALSERVER的内核中,必须编译支持IPTUNNEL这个选项｡IPTUNNEL也在Net working options里面｡\n###DR模式\n直接路由模式比较特别,很难说和什么方面相似,前2种模式基本上都是工作在网络层上(三层),而直接路由模式则应该是工作在数据链路层上(二层)｡其原理为,DR和REALSERVER都使用同一个IP对外服务｡但只有DR对ARP请求进行响应,所有REALSERVER对本身这个IP的ARP请求保持静默｡也就是说,网关会把对这个服务IP的请求全部定向给DR,而DR收到数据包后根据调度算法,找出对应的REALSERVER,把目的MAC地址改为REALSERVER的MAC并发给这台REALSERVER｡这时REALSERVER收到这个数据包,则等于直接从客户端收到这个数据包无异,处理后直接返回给客户端｡由于DR要对二层包头进行改换,所以DR和REALSERVER之间必须在一个广播域,也可以简单的理解为在同一台交换机上｡\n#LVS的算法\n###轮叫调度(Round-RobinScheduling)\n调度器通过\u0026quot;轮叫\u0026quot;调度算法将外部请求按顺序轮流分配到集群中的真实服务器上,它均等地对待每一台服务器,而不管服务器上实际的连接数和系统负载｡\n###加权轮叫调度(WeightedRound-RobinScheduling)\n调度器通过\u0026quot;加权轮叫\u0026quot;调度算法根据真实服务器的不同处理能力来调度访问请求｡这样可以保证处理能力强的服务器处理更多的访问流量｡调度器可以自动问询真实服务器的负载情况,并动态地调整其权值｡\n###最小连接调度(Least-ConnectionScheduling)\n调度器通过\u0026quot;最少连接\u0026quot;调度算法动态地将网络请求调度到已建立的链接数最少的服务器上｡如果集群系统的真实服务器具有相近的系统性能,采用\u0026quot;最小连接\u0026quot;调度算法可以较好地均衡负载｡\n###加权最小连接调度(WeightedLeast-ConnectionScheduling)\n在集群系统中的服务器性能差异较大的情况下,调度器采用\u0026quot;加权最少链接\u0026quot;调度算法优化负载均衡性能,具有较高权值的服务器将承受较大比例的活动连接负载｡调度器可以自动问询真实服务器的负载情况,并动态地调整其权值\n###基于局部性的最少链接(Locality-BasedLeastConnectionsScheduling)\n基于局部性的最少链接\u0026quot;调度算法是针对目标IP地址的负载均衡,目前主要用于Cache集群系统｡该算法根据请求的目标IP地址找出该目标IP地址最近使用的服务器,若该服务器是可用的且没有超载,将请求发送到该服务器;若服务器不存在,或者该服务器超载且有服务器处于一半的工作负载,则用\u0026quot;最少链接\u0026quot;的原则选出一个可用的服务器,将请求发送到该服务器｡\n###带复制的基于局部性最少链接(Locality-BasedLeastConnectionswithReplicationScheduling)\n带复制的基于局部性最少链接\u0026quot;调度算法也是针对目标IP地址的负载均衡,目前主要用于Cache集群系统｡它与LBLC算法的不同之处是它要维护从一个目标IP地址到一组服务器的映射,而LBLC算法维护从一个目标IP地址到一台服务器的映射｡该算法根据请求的目标IP地址找出该目标IP地址对应的服务器组,按\u0026quot;最小连接\u0026quot;原则从服务器组中选出一台服务器,若服务器没有超载,将请求发送到该服务器,若服务器超载;则按\u0026quot;最小连接\u0026quot;原则从这个集群中选出一台服务器,将该服务器加入到服务器组中,将请求发送到该服务器｡同时,当该服务器组有一段时间没有被修改,将最忙的服务器从服务器组中删除,以降低复制的程度\n###目标地址散列调度(DestinationHashingScheduling)\n目标地址散列\u0026quot;调度算法根据请求的目标IP地址,作为散列键(HashKey)从静态分配的散列表找出对应的服务器,若该服务器是可用的且未超载,将请求发送到该服务器,否则返回空\n###源地址散列调度(SourceHashingScheduling)\n源地址散列\u0026quot;调度算法根据请求的源IP地址,作为散列键(HashKey)从静态分配的散列表找出对应的服务器,若该服务器是可用的且未超载,将请求发送到该服务器,否则返回空｡\nyum install keepalived ipvsadm\r#keepalive高可用\nvim /etc/keepalived/keepalived.conf\n! Configuration File for keepalived\rglobal_defs {\rnotification_email {\racassen@firewall.loc\rfailover@firewall.loc\rsysadmin@firewall.loc\r}\rnotification_email_from Alexandre.Cassen@firewall.loc\rsmtp_server 127.0.0.1\rsmtp_connect_timeout 30\rrouter_id LVS-BACKUP\r}\rvrrp_instance VI_1 {\t#实例的ID\rstate BACKUP\t#主节点为MASTER\rinterface eth0\rvirtual_router_id 51\t# ID一定要统一两边\rpriority 50\t# 备节点一定要低于主节点\radvert_int 1\rauthentication {\tauth_type PASS\rauth_pass 1111\r}\rvirtual_ipaddress {\t192.168.56.250\t#VIP的地址\r}\r}\t主节点和主要是把角色和优先级调整一下即可，这里不再重复\n#keepalive+LVS高可用负载均衡\n还是在keepalive的配置文件末尾加入以下：\nvirtual_server 192.168.56.250 80 {\tVIP的地址\rdelay_loop 6\rlb_algo rr\t#LVS的算法（这里采用DR模式）\tlb_kind DR\rnat_mask 255.255.255.0\rpersistence_timeout 50\rprotocol TCP\rreal_server 192.168.56.11 80 {\t#后端的节点IP地址\rweight 1\rTCP_CHECK {\t# 健康检查功能\rconnect_timeout 3\rnb_get_retry 3\rdelay_before_retry 3\rconnect_port 80\r}\r}\rreal_server 192.168.56.12 80 {\rweight 1\rTCP_CHECK {\rconnect_timeout 3\rnb_get_retry 3\rdelay_before_retry 3\rconnect_port 80\r}\r}\r}\r在每个客户端的节点加入ARP的抑制\nifconfig lo:0 192.168.56.250/32 up\rroute add -host 192.168.56.250 dev lo\recho \u0026quot;1\u0026quot; \u0026gt;/proc/sys/net/ipv4/conf/lo/arp_ignore echo \u0026quot;2\u0026quot; \u0026gt;/proc/sys/net/ipv4/conf/lo/arp_announce echo \u0026quot;1\u0026quot; \u0026gt;/proc/sys/net/ipv4/conf/all/arp_ignore echo \u0026quot;2\u0026quot; \u0026gt;/proc/sys/net/ipv4/conf/all/arp_announce ","date":"2017-09-13T10:59:21+08:00","permalink":"https://caizhe.org/p/lvs-keepalived/","title":"LVS+KeepAlived"},{"content":" 在全民HTTPS的大趋势之下，Let\u0026rsquo;s Encrypt可是功不可没，网上文档也有很多，本身很简单的东西，网上文档比17年初已经多了很多很多，但是没想到水平参差不齐，部署到一半就不行了，简直是坑爹，就16年的老技术了，简单的几个步骤还能报错？！！！\n官方GitHub地址：https://github.com/Neilpang/acme.sh\n安装acme.sh脚本\n1 2 3 4 5 6 7 curl https://get.acme.sh | sh # 注册账户 ~/.acme.sh/acme.sh --register-account -m bob1317581669@gmail.com [Wed Feb 22 11:01:51 CST 2023] No EAB credentials found for ZeroSSL, let\u0026#39;s get one [Wed Feb 22 11:01:55 CST 2023] Registering account: https://acme.zerossl.com/v2/DV90 [Wed Feb 22 11:02:02 CST 2023] Registered [Wed Feb 22 11:02:03 CST 2023] ACCOUNT_THUMBPRINT=\u0026#39;yiMmaNSo-A27AG-jSqTkWrKKs7PgL7X9rPXApgxxxxx\u0026#39; 单域名证书 验证域名所有权\nacme.sh --issue -d cmdb.caizhe.org --nginx\r安装证书\n（其实证书已经下来了，就在acme的目录下面，但是你不想放在这里话，可以执行这一步）\n1 2 3 4 [root@Bob-blog conf]# acme.sh --installcert -d cmdb.caizhe.org \\ --keypath /application/nginx/conf/cmdb_ssl/cmdb.caizhe.org.key \\ --fullchainpath /application/nginx/conf/cmdb_ssl/cmdb.caizhe.org.cer \\ --reloadcmd \u0026#34;/application/nginx/sbin/nginx -s reload\u0026#34; 查看域名情况\n1 2 3 [root@Bob-blog conf]# acme.sh --list Main_Domain KeyLength SAN_Domains Created Renew cmdb.caizhe.org \u0026#34;\u0026#34; no Wed Jun 5 09:19:13 UTC 2019 Sun Aug 4 09:19:13 UTC 2019 域名更新(之后跑个定时就行了)\nacme.sh --renew -d cmdb.caizhe.org --force\r通配符证书 1 2 3 4 export Ali_Key=\u0026#34;LTAI4FyNQRVTRaGc1xxxxx\u0026#34; export Ali_Secret=\u0026#34;cE5Nhz8lRgCZ2lLxLP3ADjhIxxxxx\u0026#34; ~/.acme.sh/acme.sh --issue --dns dns_ali -d \u0026#39;caizhe.org\u0026#39; -d \u0026#39;*.caizhe.org\u0026#39; 通配符的https必须是用DNS模式，目前支持阿里DNS和DNSpod；\n如果你是用的DNS 有支持的API接口，它会自己做验证和更新，\n关于这个Let\u0026rsquo;s Encrypt 我现在已经全站使用了近两年的时间了，没什么问题，大家可以放心使用。\ncertbot 更新通配符\ncertbot-auto --server https://acme-v02.api.letsencrypt.org/directory -d \u0026quot;*.xxx.com\u0026quot; --manual --preferred-challenges dns-01 certonly\r附录Nginx配置\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 server { listen 80; server_name cmdb.caizhe.org; location /.well-known { alias /application/nginx/html/cmdb/.well-known; } rewrite ^(.*) https://$server_name$1 last; error_page 497 https://$server_name$request_uri; } server { listen 443 ssl; server_name cmdb.caizhe.org; ssl_certificate /root/.acme.sh/cmdb.caizhe.org/cmdb.caizhe.org.cer; ssl_certificate_key /root/.acme.sh/cmdb.caizhe.org/cmdb.caizhe.org.key; ssl_ciphers EECDH+CHACHA20:EECDH+CHACHA20-draft:EECDH+AES128:RSA+AES128:EECDH+AES256:RSA+AES256::!MD5; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on; #ssl_session_cache builtin:1000 shared:SSL:10m; #ssl_session_timeout 1d; #ssl_session_tickets on; location / { root html/cmdb; index index.html index.htm; } } ","date":"2017-06-13T10:59:21+08:00","permalink":"https://caizhe.org/p/letsencrypt/","title":"LetsEncrypt"},{"content":"Redis 持久化方式 Redis 提供了多种不同级别的持久化方式：\nRDB 持久化可以在指定的时间间隔内生成数据集的时间点快照（point-in-time snapshot）。\rAOF AOF 持久化记录服务器执行的所有写操作命令，并在服务器启动时，通过重新执行这些命令来还原数据集。\tRedis 还可以同时使用 AOF 持久化和 RDB 持久化。 在这种情况下， 当 Redis 重启时， 它会优先使用 AOF 文件来还原数据集， 因为 AOF 文件保存的数据集通常比 RDB 文件所保存的数据集更完整。\nRDB 优缺点 优：故障恢复的时候特别快，最大化Redis性能；直接fork一个子进程，之后的所有保存工作全交给子进程；支持的功能更多。\n缺：如果在未保存快照的时候故障，会丢失部分数据；子进程在fork的时候会非常影响性能；可能会造成客户端连接的停顿。\nAOF 优缺点 优：数据相比RDB可能更完整一些，因为是每秒写入一次，最多丢一秒的数据；操作追加到日志文件，可重写； 持久化记录服务器执行的所有写操作命令，并在服务器启动时，通过重新执行这些命令来还原数据集。新命令会被追加到文件的末尾。 Redis 还可以在后台对 AOF 文件进行重写（rewrite），使得 AOF 文件的体积不会超出保存数据集状态所需的实际大小。 （什么是重写：set a 1，set a 2 set a 3，只记录set a 3）\n缺：体积大于RDB，恢复速度慢于RDB\nRrdis 主从原理 1、当从库和主库建立MS关系后，会向主数据库发送PSYNC命令；\r2、主库接收到PSYNC命令后会开始在后台保存快照（RDB持久化过程），并将期间接收到的写请求写到缓存区；\r3、当快照完成后，主Redis会将快照文件和所有缓存的写命令发送给从Redis；\r4、从Redis接收到后，会载入快照文件并且执行收到的缓存的命令；\r5、主Redis每当接收到写命令时就会将命令发送从Redis，从而保证数据的一致；\rRedis 优化 更改端口\nport 6379\t设置密码\nrequirepass XXXXXX\r绑定地址\nbind 10.0.0.10\r后台运行\ndaemonize yes\r日志级别\nloglevel warning\r日志位置\nlogfile \u0026quot;/var/log/redis_6379.log\u0026quot;\r持久化\n建议master关闭，slave开启\nsave 900 1\rsave 300 10\rsave 60 10000 开启只读\nslave-read-only yes\r慢查询\nslowlog-log-slower-than 5000\r最大内存\nmaxmemory 5gb\r数据淘汰机制 volatile-lru：使用LRU算法进行数据淘汰（淘汰上次使用时间最早的，且使用次数最少的key），只淘汰设定了有效期的key\rallkeys-lru：使用LRU算法进行数据淘汰，所有的key都可以被淘汰\rvolatile-random：随机淘汰数据，只淘汰设定了有效期的key\rallkeys-random：随机淘汰数据，所有的key都可以被淘汰\rvolatile-ttl：淘汰剩余有效期最短的key\r","date":"2017-03-02T20:52:54+08:00","permalink":"https://caizhe.org/p/redis%E4%BC%98%E5%8C%96/","title":"Redis优化"},{"content":"1）301和302的区别。\n301和302状态码都表示重定向，就是说浏览器在拿到服务器返回的这个状态码后会自动跳转到一个新的URL地址，这个地址可以从响应的Location首部中获取 （用户看到的效果就是他输入的地址A瞬间变成了另一个地址B）——这是它们的共同点。\n他们的不同在于。301表示旧地址A的资源已经被永久地移除了（这个资源不可访问了），搜索引擎在抓取新内容的同时也将旧的网址交换为重定向之后的网址；\n302表示旧地址A的资源还在（仍然可以访问），这个重定向只是临时地从旧地址A跳转到地址B，搜索引擎会抓取新的内容而保存旧的网址。 SEO302好于301\n2）重定向原因： （1）网站调整（如改变网页目录结构）； （2）网页被移到一个新地址； （3）网页扩展名改变(如应用需要把.php改成.Html或.shtml)。 这种情况下，如果不做重定向，则用户收藏夹或搜索引擎数据库中旧地址只能让访问客户得到一个404页面错误信息，访问流量白白丧失；再者某些注册了多个域名的 网站，也需要通过重定向让访问这些域名的用户自动跳转到主站点等。\n","date":"2016-12-11T10:59:21+08:00","permalink":"https://caizhe.org/p/301%E5%92%8C302%E5%8C%BA%E5%88%AB/","title":"301和302区别"},{"content":"最近闲着无聊，对比了docker和KVM，发现docker很厉害啊，未来的成长空间非常不错，但是目前还是有些不足，比如一些对于隔离的效果不是很好，再等两年成熟之后再去研究，现在感觉还是KVM比较实用，玩了玩整理了份文档，供大家分享：\n###基础环境 [root@localhost ~]# cat /etc/redhat-release CentOS Linux release 7.1.1503 (Core) [root@localhost ~]# uname -r 3.10.0-229.el7.x86_64\n###查看是否支持\ngrep -E '(vmx|svm)' /proc/cpuinfo\r###安装\nyum install qemu-kvm libvirt libvirt-python libguestfs-tools virt-install qemu-kvm-toools virt-manager\r###启动\nsystemctl start libvirtd\rsystemctl status libvirtd\r###创建虚拟机\nqemu-img create -f raw Centos-7.raw 10G\t#在当前目录创建一个格式为raw的空的虚拟机，大小为10G\rvirt-install --name Centos-7 --ram 1024 --vcpus=1 --disk path=Centos-7.raw --cdrom=CentOS-7-x86_64-DVD-1503-01_2.iso --graphics vnc,listen=0.0.0.0 使用VNC工具连接到主机即可（安装操作系统这里不再介绍）\n查看运行中的虚拟机\nvirsh list --all\rId Name State\r----------------------------------------------------\r7 Centos-7 running\r查看虚拟机的详细信息\n[root@localhost media]# virsh dominfo Centos-7\rId: 7\rName: Centos-7\rUUID: 544cdb00-6c2c-4c93-9e11-086920b89320\rOS Type: hvm\rState: running\rCPU(s): 1\rCPU time: 46.9s\rMax memory: 1048576 KiB\rUsed memory: 1048576 KiB\rPersistent: yes\rAutostart: disable\rManaged save: no\rSecurity model: none\rSecurity DOI: 0\t##KVM的扩容与缩容\n####内存\nvirsh setmem Centos-7 512M\r####硬盘\n硬盘也可以动态扩容、缩容，但是笔者经验不建议，可能丢失数据，如果数据硬盘不够了，再重新添加一块硬盘。\n##格式转换\n[root@linux-node2 opt]# qemu-img convert -f raw -O qcow2 Centos-7.raw Centos-7.qcow2\r[root@linux-node2 opt]# ll -h Centos-7.*\r-rw-r--r-- 1 root root 1017M May 4 16:18 Centos-7.qcow2\r-rw-r--r-- 1 qemu qemu 10G May 4 16:11 Centos-7.raw\r#RAW与QCOW2优缺点：\nraw格式是最原始的一种格式，特点是I/O性能最好的一种格式，没有之一，缺点就是功能很少，有很多快照、加密、压缩都不支持，而且特别大，最开始制定10G就占用10G的空间。\rqcow2与raw相反，功能很多，支持快照、加密、压缩，而且占用资源小，在原始的镜像上面只保留修改的部分，但是性能差于raw。\r##让虚拟机桥接到物理网卡\n此时的虚拟机还不能上网，仅仅是运行了一个系统，要想上网进行如下操作：\nbrctl addbr br0\r模拟出一块网卡名字为 br0\nbrctl addif br0 eth0\r桥接到eth0，注意，这样会离开断开与服务器的连接，因为服务器的eth0是没有IP地址的\nip addr del dev eth0 192.168.56.11/24\r删除原eth0的IP地址\nifconfig br0 192.168.56.11/24 up\r此时虽然连上了服务器，但是任然不能上网，因为路由不对，添加以下：\nroute add default gw 1.1.1.1（网关地址）\r现在会断开与KVM虚拟机的连接，没关系，修改虚拟机的XML文件：\nvirsh edit Centos-7\t#修改为以下两行\r\u0026lt;interface type='bridge'\u0026gt;\r\u0026lt;source bridge='br0'/\u0026gt;\r重启一下\nvirsh reboot Centos-7\r然后配置IP地址、网关、DNS等即可连接上网，此处不再演示\n附带一些常用命令：\n开机：\nvirsh start test1\r关机：\nvirsh shutdown test1\r强制关机：\nvirsh destroy test1\r重新启动：\nvirsh reboot test1\r通过配置文档启动主机：\nvirsh create /etc/libvirt/qemu/test1.xml\r查看主机状态：\nvirsh list --all\r停止/挂机虚拟机：\nvirsh suspend test1\r保存虚拟机：\nvirsh save test1 或者\rvirsh dumpxml Centos-7 \u0026gt; dump_kvm.xml\r还原虚拟机：\nvirsh resume test1\r#快照功能\n默认快照文件放在 /var/lib/libvirt/qemu/snapshot/\n拍摄快照：\nvirsh snapshot-create test1\r查看快照：\nvirsh snapshot-list test1\rName Creation Time State\r------------------------------------------------------------\r1486394873 2017-02-06 23:27:53 +0800 shutoff\t#上次快照\r1486394993 2017-02-06 23:29:53 +0800 shutoff\t#这次快照\r恢复快照：\nvirsh snapshot-revert test1 1486394873\r例1：修改CPU数量\nvirsh edit Centos-7\r\u0026lt;vcpu placement='auto' current='1'\u0026gt;4\u0026lt;/vcpu\u0026gt;\t#CPU上限是4个，目前是一个\rvirsh setvcpus Centos-7 2 --live\t#也可以使用--config参数\rvirsh reboot Centos-7\t#重启可选\r例2：添加硬盘\nqemu-img create -f qcow2 add.img 20G\rvirsh attach-disk Centos-7 /opt/add.img sdb\t#添加一块硬盘（热添加，重启失效）\rvirsh edit Centos-7\t#写入配置文件\r…………\r\u0026lt;/disk\u0026gt;\r\u0026lt;disk type='file' device='disk'\u0026gt;\r\u0026lt;driver name='qemu' type='qcow2' cache='none'/\u0026gt;\r\u0026lt;source file='/opt/add.img'/\u0026gt;\r\u0026lt;target dev='sdb' bus='virtio'/\u0026gt;\r\u0026lt;address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/\u0026gt;\r\u0026lt;/disk\u0026gt;\rvirsh reboot Centos-7\r后来我有找到更简单的一种方法：\nvirsh attach-disk Centos-7 /opt/add.img sdb --config ","date":"2016-05-02T17:44:11+08:00","permalink":"https://caizhe.org/p/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA/","title":"KVM虚拟机"},{"content":"系统优化 1.精简开机自启动\n2.禁止ROOT远程登录，更改SSH端口，远程登录改为秘钥认证，如果有必要换成VPN内网连接。\n3.根据业务关闭IPtables、关闭Selinux。\n4.更换国内较快的YUM源\n5.调整文件字符集，最好为UTF-8\n7.修改系统登录信息 /etc/issue\n8.时间同步\n9.设置连接终端超时\n10.不要使用IP地址，尽量使用主机名。\n11.锁文件（慎用）\n6.调整文件描述符大小\nvim /etc/security/limits.conf\r* soft nofile 65535\r* hard nofile 65535 内核优化 开启内核转发\nnet.ipv4.ip_forward = 1\r设置timewait的数量，默认180000\nnet.ipve.tcp_max_tw_buckets = 6000\r用来设定允许系统打开的端口范围。\nnet.ipv4.ip_local_port_range = 1024 65000\r设置启用timewait快速回收。\nnet.ipv4.tcp_tw_recycle = 1\r用于设置开启重用，允许将TIME-WAIT sockets重新用于新的TCP连接\nnet.ipv4.tcp_tw_reuse = 1\r开启SYN Cookies，当出现SYN等待队列溢出时，启用cookies进行处理。\nnet.ipv4.tcp_syncookies = 1\r用于调节系统同时发起的tcp连接数，在高并发的请求中，默认的值可能会导致链接超时或者重传，因此，需要结合并发请求数来调节此值。默认值是128\nnet.core.somaxconn = 262144\r当每个网络接口接收数据包的速率比内核处理这些包的速率快时，允许发送到队列的数据包的最大数目。\nnet.core.netdev_max_backlog = 262144\r用于设定系统中最多有多少个TCP套接字不被关联到任何一个用户文件句柄上。如果超过这个数字，孤立连接将立即被复位并打印出警告信息。这个限制只是为了防止简单的DoS攻击。不能过分依靠这个限制甚至人为减小这个值，更多的情况下应该增加这个值\nnet.ipv4.tcp_max_orphans = 262144\r记录那些尚未收到客户端确认信息的连接请求的最大值。对于有128MB内存的系统而言，此参数的默认值是1024，对小内存的系统则是128\nnet.ipv4.tcp_max_syn_backlog = 262144\r内核放弃连接之前发送SYN+ACK包的数量。\nnet.ipv4.tcp_synack_retries = 1\r在内核放弃建立连接之前发送SYN包的数量。\nnet.ipv4.tcp_syn_retries = 1\r设置套接字保持在FIN-WAIT-2状态的时间。默认值是60秒\nnet.ipv4.tcp_fin_timeout = 1\r当长连接启用的时候，TCP发送长连接消息的频度。默认值是2小时\nnet.ipv4.tcp_keepalive_time = 30\r调整使用swap分区的使用率/百分百。(100-10)90，当内存使用率达到90使用swap，redis服务器建议为0\n/proc/sys/vm/swappiness 10\r这篇文章较短，未来会不断完善，欢迎大家提供建议。\n","date":"2016-03-02T20:49:41+08:00","permalink":"https://caizhe.org/p/linux%E7%B3%BB%E7%BB%9F%E4%BC%98%E5%8C%96/","title":"linux系统优化"},{"content":"nginx 的优化分为两个小方向：性能优化 \u0026amp; 安全优化\n性能优化 开启gzip压缩\ngzip on;\rgzip_min_length 1k;\rgzip_buffers 4 32k;\rgzip_http_version 1.1;\rgzip_comp_level 2;\rgzip_types text/css text/js text/xml application/javascript;\rgzip_vary on;\r使用expires 缓存\nlocation ~^/(images|javascript|js|css|flash|media|static)/ {\rexpires 30d;\r}\rlocation ~ .*\\.(gif|jpg|jpeg|png|bmp|swf)$\r{\rexpires 3650d;\r}\r使用epoll模型\nuse epoll;\r配置多个work进程\n与CPU的核数相等 or CPU的核数 X2\r调整当进程的最大连接数\nworker_connections 20480；\r调整进程可打开的的最大文件数\nworker_rlimit_nofile 65535;\r设置连接超时\nkeepalive_timeout 300;\rJAVA应用经量使用长连接、如果是PHP的尽量使用短连接，因为Java应用要消耗的资源和时间要比PHP跟多（具体依然要看业务）\r客户端请求头超时时间/大小\nclient_header_timeout 15s;\tclient_header_buffer_size 10k;\r客户端请求主体超时时间\nclient_header_timeout 15s;\r用户上传大小的限制\nclient_max_body_size 10m;\r开启高效文件传输模式\nsendfile on;\rtcp_nopush on;\raio on;\r为后端代理服务器设置缓冲区\nproxy_buffering on;\rproxy_buffer_size 64k;\rproxy_buffers 4 32k;\rproxy_busy_buffers_size 64k;\rproxy_temp_file_write_size 64k;\r制定客户端的超时时间\nsend_timeout 300s;\r安全优化 隐藏版本号\nserver_tokens off;\r更改默认用户组\nuser nginx;\r静止解析制定目录下的特定文件\nlocation ~ ^/images/.*\\.(php|php5|.sh|.pl|.py)$ { deny all; } 限制及指定IP或IP段访问\nlocation / { deny 192.168.1.1; allow 192.168.1.0/24; deny all; }\r限制非法IP\nif ($remote_addr = 10.0.0.2 ) {\rreturn 403;\r}\r禁止非法域名解析访问\nif （$host !~ caizhe/.org$）{\rreturn 500;\r}\rserver {\rlisten 80 default;\rreturn 500;\r}\r限制访问频率\nhttp {\tlimit_req_zone $binary_remote_addr zone=ttlsa_com:10m rate=1r/s;\rserver {\rlimit_req zone=one burst=5;\r错误页面优雅显示\nerror_page 500 501 502 503 504 https://caizhe.org;\rerror_page 400 403 404 405 408 410 411 412 413 414 415 https://caizhe.org;\r使用普通用户启动nginx\n","date":"2016-03-02T20:46:18+08:00","permalink":"https://caizhe.org/p/nginx%E4%BC%98%E5%8C%96/","title":"Nginx优化"}]