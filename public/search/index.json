[{"content":"环境信息 系统版本 Docker LDAP Ubuntu 18 20.10 2.4.57 安装OpenLDAP 1 2 3 4 5 6 7 8 docker run -d \\ -p 389:389 \\ --name openldap \\ --restart=always \\ --env LDAP_ORGANISATION=\u0026#34;caizhe\u0026#34; \\ --env LDAP_DOMAIN=\u0026#34;caizhe.org\u0026#34; \\ --env LDAP_ADMIN_PASSWORD=\u0026#34;111111\u0026#34; \\ osixia/openldap 安装web管理端（PHPLDAPadmin） 1 2 3 4 5 6 7 docker run -d \\ --privileged \\ -p 80:80 \\ --name phpldapadmin \\ --env PHPLDAPADMIN_HTTPS=false \\ --env PHPLDAPADMIN_LDAP_HOSTS=192.168.1.100 \\ osixia/phpldapadmin 测试命令： 1 ldapsearch -x -H ldap:/// -D \u0026#34;cn=admin,dc=caizhe,dc=org\u0026#34; -w 111111 -b \u0026#34;dc=caizhe,dc=org\u0026#34; -LLL 或者在容器里执行\n1 ldapsearch -x -H ldap:/// -D \u0026#34;cn=admin,dc=caizhe,dc=org\u0026#34; -w 111111 -b \u0026#34;dc=caizhe,dc=org\u0026#34; -LLL 创建组 LDAP新建用户\n建立用户之前先建立一个group\n选择default\n这里可以随便选一个单位，后期还要改！\n确认无误提交即可，组算是创建完毕了\n创建用户 LDAP组和人员之前默认是不关联的，我们需要把刚才新建的用户添加到“dba-group”组里\n最终的结果是这样的，“Update~”提交即可\n用户已经加入这个组了，我们来看一下，重点是memberOf，咱们Jenkins对组授权就是通过读取memberOf这个字段来判断的，（很想问问作者为什么不能去读取DN，根据OU来判断）\n密码策略 密码策略是一般是使用ppolicy，可以使用下面命令查看是否加载了ppolicy模块\n1 2 3 4 5 6 7 8 9 10 11 12 root@openldap-host:/etc/ldap# slapcat -n 0 | grep -i module dn: cn=module{0},cn=config objectClass: olcModuleList cn: module{0} olcModulePath: /usr/lib/ldap olcModuleLoad: {0}back_mdb olcModuleLoad: {1}memberof olcModuleLoad: {2}refint structuralObjectClass: olcModuleList olcAttributeTypes: {15}( 1.3.6.1.4.1.4754.1.99.1 NAME \u0026#39;pwdCheckModule\u0026#39; DESC \u0026#39;Loadable module that instantiates \u0026#34;check_password() function\u0026#39; EQUALITY cas op AUXILIARY MAY pwdCheckModule ) 没有的话需要ldapadd命令额外加载模块，新建一个ldif文件\n1 2 3 4 5 cat load-ppolicy-mod.ldif dn: cn=module{0},cn=config changetype: modify add: olcModuleLoad olcModuleLoad: ppolicy.la 加载命令：\n1 ldapadd -Y EXTERNAL -H ldapi:/// -f load-ppolicy-mod.ldif 检查是否加载成功\n1 2 3 4 5 6 7 8 9 10 11 12 13 root@openldap-host:/etc/ldap# slapcat -n 0 | grep -i module dn: cn=module{0},cn=config objectClass: olcModuleList cn: module{0} olcModulePath: /usr/lib/ldap olcModuleLoad: {0}back_mdb olcModuleLoad: {1}memberof olcModuleLoad: {2}refint olcModuleLoad: {3}ppolicy.la structuralObjectClass: olcModuleList olcAttributeTypes: {15}( 1.3.6.1.4.1.4754.1.99.1 NAME \u0026#39;pwdCheckModule\u0026#39; DESC \u0026#39;Loadable module that instantiates \u0026#34;check_password() function\u0026#39; EQUALITY cas op AUXILIARY MAY pwdCheckModule ) 检查现有LDAP策略的数据库存储格式，发现均为mdb格式\n1 2 3 4 5 6 7 8 root@openldap-host:/etc/ldap# ldapsearch -LLL -Y EXTERNAL -H ldapi:/// -b cn=config olcDatabase | grep mdb SASL/EXTERNAL authentication started SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth SASL SSF: 0 dn: olcDatabase={1}mdb,cn=config olcDatabase: {1}mdb dn: olcOverlay={0}memberof,olcDatabase={1}mdb,cn=config dn: olcOverlay={1}refint,olcDatabase={1}mdb,cn=config 创建默认密码策略的DN\n1 2 3 4 5 6 7 root@openldap-host:/etc/ldap# cat pwpolicyoverlay.ldif dn: olcOverlay=ppolicy,olcDatabase={1}mdb,cn=config objectClass: olcOverlayConfig objectClass: olcPPolicyConfig olcOverlay: ppolicy olcPPolicyDefault: cn=default,dc=caizhe,dc=org olcPPolicyHashCleartext: TRUE 更新数据库\n1 2 3 4 5 root@498d5b127557:/# ldapadd -Y EXTERNAL -H ldapi:/// -f pwpolicyoverlay.ldif SASL/EXTERNAL authentication started SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth SASL SSF: 0 adding new entry \u0026#34;olcOverlay=ppolicy,olcDatabase={1}mdb,cn=config\u0026#34; 添加默认策略规则\n这里为了测试（密码过期设置了100秒，60秒前会有提醒）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 cat ldap-pwpolicies.ldif dn: cn=default,dc=caizhe,dc=org objectClass: inetOrgPerson objectClass: pwdPolicyChecker objectClass: pwdPolicy cn: pwpolicy sn: pwpolicy pwdAttribute: userPassword pwdMinAge: 0 pwdMaxAge: 100 pwdInHistory: 5 pwdCheckQuality: 2 pwdMinLength: 8 pwdExpireWarning: 60 pwdGraceAuthNLimit: 3 pwdLockout: TRUE pwdLockoutDuration: 0 pwdMaxFailure: 0 pwdFailureCountInterval: 0 pwdReset: TRUE pwdMustChange: TRUE pwdAllowUserChange: TRUE pwdSafeModify: FALSE 更新 slapd 上的密码策略(创建一个默认的策略，名字叫default)\n1 2 3 4 5 ldapadd -Y EXTERNAL -H ldapi:/// -f ldap-pwpolicies.ldif SASL/EXTERNAL authentication started SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth SASL SSF: 0 adding new entry \u0026#34;cn=default,dc=caizhe,dc=org\u0026#34; 测试 修改密码为不符合要求：\n1 2 3 4 5 6 7 8 root@openldap-host:/etc/ldap# ldappasswd -H ldapi:/// -Y EXTERNAL -S \u0026#34;cn=ccc,cn=default,ou=pwpolicy,dc=caizhe,dc=org\u0026#34; New password: Re-enter new password: SASL/EXTERNAL authentication started SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth SASL SSF: 0 Result: Constraint violation (19) Additional info: Password fails quality checking policy 新建用户登录测试密码有效期：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 64c1e4ea conn=1068 fd=14 ACCEPT from IP=192.168.0.131:50662 (IP=0.0.0.0:389) 64c1e4ea conn=1068 op=0 BIND dn=\u0026#34;cn=aaa,dc=caizhe,dc=org\u0026#34; method=128 64c1e4ea conn=1068 op=0 BIND dn=\u0026#34;cn=aaa,dc=caizhe,dc=org\u0026#34; mech=SIMPLE ssf=0 64c1e4ea ppolicy_bind: Setting warning for password expiry for cn=aaa,dc=caizhe,dc=org = 21 seconds\t#还有21秒过期 64c1e4ea conn=1068 op=0 RESULT tag=97 err=0 text= 64c1e4ea conn=1068 op=1 SRCH base=\u0026#34;\u0026#34; scope=0 deref=3 filter=\u0026#34;(objectClass=)\u0026#34; 64c1e4ea conn=1068 op=1 SRCH attr=subschemaSubentry 64c1e4ea conn=1068 op=1 SEARCH RESULT tag=101 err=0 nentries=1 text= 64c1e4ea conn=1068 op=2 SRCH base=\u0026#34;cn=Subschema\u0026#34; scope=0 deref=3 filter=\u0026#34;(objectClass=subschema)\u0026#34; 64c1e4ea conn=1068 op=2 SRCH attr=createTimestamp modifyTimestamp 64c1e4ea conn=1068 op=2 SEARCH RESULT tag=101 err=0 nentries=1 text= 64c1e4ea conn=1068 op=3 SRCH base=\u0026#34;\u0026#34; scope=0 deref=0 filter=\u0026#34;(objectClass=)\u0026#34; 64c1e4ea conn=1068 op=3 SRCH attr=namingContexts subschemaSubentry supportedLDAPVersion supportedSASLMechanisms supportedExtension supportedControl supportedFeatures vendorName vendorVersion + objectClass 64c1e4ea conn=1068 op=3 SEARCH RESULT tag=101 err=0 nentries=1 text= 64c1e4ea conn=1068 op=4 SRCH base=\u0026#34;\u0026#34; scope=0 deref=0 filter=\u0026#34;(objectClass=)\u0026#34; 64c1e4ea conn=1068 op=4 SRCH attr= 64c1e4ea conn=1068 op=4 SEARCH RESULT tag=101 err=0 nentries=1 text= 64c1e4ea conn=1068 op=5 SRCH base=\u0026#34;cn=Subschema\u0026#34; scope=0 deref=3 filter=\u0026#34;(objectClass=)\u0026#34; 64c1e4ea conn=1068 op=5 SRCH attr=hasSubordinates objectClass 64c1e4ea conn=1068 op=5 SEARCH RESULT tag=101 err=0 nentries=1 text= 64c1e4ea conn=1068 op=6 SRCH base=\u0026#34;dc=caizhe,dc=org\u0026#34; scope=0 deref=3 filter=\u0026#34;(objectClass=)\u0026#34; 64c1e4ea conn=1068 op=6 SRCH attr=hasSubordinates objectClass 64c1e4ea conn=1068 op=6 SEARCH RESULT tag=101 err=32 nentries=0 text= 64c1e4ea conn=1068 op=7 SRCH base=\u0026#34;cn=config\u0026#34; scope=0 deref=3 filter=\u0026#34;(objectClass=*)\u0026#34; 64c1e4ea conn=1068 op=7 SRCH attr=hasSubordinates objectClass 64c1e4ea conn=1068 op=7 SEARCH RESULT tag=101 err=32 nentries=0 text= 注意：原有账户的密码不受影响，只有后面新加的用户会收到密码策略的限制\n自主改密平台 创建一个配置文件\n1 2 3 4 5 6 7 8 9 10 11 cat config.inc.php \u0026lt;?php // My SSP configuration $keyphrase = \u0026#34;mysecret\u0026#34;; $debug = false; $use_captcha = true; $ldap_url = \u0026#34;ldap://192.168.1.250:389\u0026#34;; $ldap_binddn = \u0026#34;CN=admin,DC=caizhe,DC=org\u0026#34;; $ldap_bindpw = \u0026#34;111111\u0026#34;; $ldap_base = \u0026#34;dc=caizhe,dc=org\u0026#34;; $ldap_filter = \u0026#34;(\u0026amp;(objectClass=inetOrgPerson)(cn={login}))\u0026#34;; ?\u0026gt; 运行新的容器\ndocker run -p 9080:80 --restart=always --name selfServicePassword -v $PWD/config.inc.php:/var/www/conf/config.inc.local.php -itd docker.io/ltbproject/self-service-password:1.5 直接修改密码即可\n参考链接：\nhttps://www.openldap.org/devel/admin/overlays.html https://tutoriels.meddeb.net/openldap-password-policy-managing-users-accounts/\nhttps://kifarunix.com/implement-openldap-password-policies/\nhttps://tylersguides.com/guides/openldap-password-policy-overlay/\n","date":"2023-07-24T14:29:17+08:00","permalink":"https://caizhe.org/p/docker%E5%AE%89%E8%A3%85openldap/","title":"Docker安装OpenLDAP"},{"content":"Redis 5.0.7\n创建集群目录\nmkdir /home/redis-cluster\r1 2 3 4 5 6 7 8 9 port ${PORT} protected-mode no cluster-enabled yes cluster-config-file nodes.conf cluster-node-timeout 5000 cluster-announce-ip 192.168.21.28 //自己服务器IP cluster-announce-port ${PORT} cluster-announce-bus-port 1${PORT} appendonly yes 创建专有网络（可选）\ndocker network create redis-net\r创建conf和data目录\n1 2 3 4 5 for port in `seq 7000 7005`; do \\ mkdir -p ./${port}/conf \\ \u0026amp;\u0026amp; PORT=${port} envsubst \u0026lt; ./redis-cluster.tmpl \u0026gt; ./${port}/conf/redis.conf \\ \u0026amp;\u0026amp; mkdir -p ./${port}/data; \\ done 创建Redis容器\n1 2 3 4 5 6 7 for port in `seq 7000 7005`; do \\ docker run -d -ti -p ${port}:${port} -p 1${port}:1${port} \\ -v /home/redis-cluster/${port}/conf/redis.conf:/usr/local/etc/redis/redis.conf \\ -v /home/redis-cluster/${port}/data:/data \\ --restart always --name redis-${port} --net redis-net \\ --sysctl net.core.somaxconn=1024 redis redis-server /usr/local/etc/redis/redis.conf; \\ done 随意进入一个容器\ndocker exec -it e61133e98d01 /bin/bash\r创建集群\nredis-cli --cluster create 192.168.21.28:7000 192.168.21.28:7001 192.168.21.28:7002 192.168.21.28:7002 192.168.21.28:7003 192.168.21.28:7004 192.168.21.28:7005 --cluster-replicas 1\r输出记录\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 \u0026gt;\u0026gt;\u0026gt; Performing hash slots allocation on 7 nodes... Master[0] -\u0026gt; Slots 0 - 5460 Master[1] -\u0026gt; Slots 5461 - 10922 Master[2] -\u0026gt; Slots 10923 - 16383 Adding replica 192.168.21.28:7003 to 192.168.21.28:7000 Adding replica 192.168.21.28:7004 to 192.168.21.28:7001 Adding replica 192.168.21.28:7005 to 192.168.21.28:7002 Adding extra replicas... Adding replica 192.168.21.28:7002 to 192.168.21.28:7000 \u0026gt;\u0026gt;\u0026gt; Trying to optimize slaves allocation for anti-affinity [WARNING] Some slaves are in the same host as their master M: d4b82300dfacedeaf7d50e0c3c06d9b7662603e1 192.168.21.28:7000 slots:[0-5460] (5461 slots) master M: f89d211a74b1e71b4b3aa12ae9491cd0657c9f06 192.168.21.28:7001 slots:[5461-10922] (5462 slots) master M: d02714dc30ed747bc23d8387684d0389b1552a61 192.168.21.28:7002 slots:[10923-16383] (5461 slots) master S: d02714dc30ed747bc23d8387684d0389b1552a61 192.168.21.28:7002 replicates f89d211a74b1e71b4b3aa12ae9491cd0657c9f06 S: 5188757b51c2fc804ab9a09779cc83f3dd08e7e2 192.168.21.28:7003 replicates d4b82300dfacedeaf7d50e0c3c06d9b7662603e1 S: 985e81724ea9b5664a19961f4a758689290856b2 192.168.21.28:7004 replicates d02714dc30ed747bc23d8387684d0389b1552a61 S: 2f12b2eb90938cf2469bd6209f85dc42925e2129 192.168.21.28:7005 replicates d4b82300dfacedeaf7d50e0c3c06d9b7662603e1 Can I set the above configuration? (type \u0026#39;yes\u0026#39; to accept): yes \u0026gt;\u0026gt;\u0026gt; Nodes configuration updated \u0026gt;\u0026gt;\u0026gt; Assign a different config epoch to each node \u0026gt;\u0026gt;\u0026gt; Sending CLUSTER MEET messages to join the cluster Waiting for the cluster to join .... \u0026gt;\u0026gt;\u0026gt; Performing Cluster Check (using node 192.168.21.28:7000) M: d4b82300dfacedeaf7d50e0c3c06d9b7662603e1 192.168.21.28:7000 slots:[0-5460] (5461 slots) master 2 additional replica(s) M: d02714dc30ed747bc23d8387684d0389b1552a61 192.168.21.28:7002 slots:[10923-16383] (5461 slots) master 1 additional replica(s) S: 985e81724ea9b5664a19961f4a758689290856b2 192.168.21.28:7004 slots: (0 slots) slave replicates d02714dc30ed747bc23d8387684d0389b1552a61 S: 2f12b2eb90938cf2469bd6209f85dc42925e2129 192.168.21.28:7005 slots: (0 slots) slave replicates d4b82300dfacedeaf7d50e0c3c06d9b7662603e1 S: 5188757b51c2fc804ab9a09779cc83f3dd08e7e2 192.168.21.28:7003 slots: (0 slots) slave replicates d4b82300dfacedeaf7d50e0c3c06d9b7662603e1 M: f89d211a74b1e71b4b3aa12ae9491cd0657c9f06 192.168.21.28:7001 slots:[5461-10922] (5462 slots) master [OK] All nodes agree about slots configuration. \u0026gt;\u0026gt;\u0026gt; Check for open slots... \u0026gt;\u0026gt;\u0026gt; Check slots coverage... [OK] All 16384 slots covered. 连接测试\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 e61133e98d01 redis \u0026#34;docker-entrypoint.s…\u0026#34; 25 minutes ago Up 25 minutes 0.0.0.0:7005-\u0026gt;7005/tcp, 6379/tcp, 0.0.0.0:17005-\u0026gt;17005/tcp redis-7005 4860f478463f redis \u0026#34;docker-entrypoint.s…\u0026#34; 25 minutes ago Up 25 minutes 0.0.0.0:7004-\u0026gt;7004/tcp, 6379/tcp, 0.0.0.0:17004-\u0026gt;17004/tcp redis-7004 16d6bb61118b redis \u0026#34;docker-entrypoint.s…\u0026#34; 25 minutes ago Up 25 minutes 0.0.0.0:7003-\u0026gt;7003/tcp, 6379/tcp, 0.0.0.0:17003-\u0026gt;17003/tcp redis-7003 7b6070c17724 redis \u0026#34;docker-entrypoint.s…\u0026#34; 25 minutes ago Up 25 minutes 0.0.0.0:7002-\u0026gt;7002/tcp, 6379/tcp, 0.0.0.0:17002-\u0026gt;17002/tcp redis-7002 afd85937ac9a redis \u0026#34;docker-entrypoint.s…\u0026#34; 25 minutes ago Up 25 minutes 0.0.0.0:7001-\u0026gt;7001/tcp, 6379/tcp, 0.0.0.0:17001-\u0026gt;17001/tcp redis-7001 0298db107972 redis \u0026#34;docker-entrypoint.s…\u0026#34; 25 minutes ago Up 25 minutes 0.0.0.0:7000-\u0026gt;7000/tcp, 6379/tcp, 0.0.0.0:17000-\u0026gt;17000/tcp redis-7000 redis-cli -c -h 192.168.21.28 -p 7002 192.168.21.28:7002\u0026gt; CLUSTER NODES 985e81724ea9b5664a19961f4a758689290856b2 192.168.21.28:7004@17004 slave d02714dc30ed747bc23d8387684d0389b1552a61 0 1576224546000 6 connected 2f12b2eb90938cf2469bd6209f85dc42925e2129 192.168.21.28:7005@17005 slave d4b82300dfacedeaf7d50e0c3c06d9b7662603e1 0 1576224547233 1 connected 5188757b51c2fc804ab9a09779cc83f3dd08e7e2 192.168.21.28:7003@17003 slave d4b82300dfacedeaf7d50e0c3c06d9b7662603e1 0 1576224546227 5 connected f89d211a74b1e71b4b3aa12ae9491cd0657c9f06 192.168.21.28:7001@17001 master - 0 1576224546000 2 connected 5461-10922 d02714dc30ed747bc23d8387684d0389b1552a61 192.168.21.28:7002@17002 myself,master - 0 1576224544000 3 connected 10923-16383 d4b82300dfacedeaf7d50e0c3c06d9b7662603e1 192.168.21.28:7000@17000 master - 0 1576224547000 1 connected 0-5460 参考链接：\nhttps://blog.csdn.net/tszxlzc/article/details/86565327\nhttps://www.cnblogs.com/lianggp/articles/8136222.html\n","date":"2023-06-14T14:29:17+08:00","permalink":"https://caizhe.org/p/docker%E5%AE%89%E8%A3%85redis-cluster/","title":"Docker安装Redis Cluster"},{"content":"apiserver调用分为3种：\n使用官方的SDK 在Pod内部使用Token，使用443端口，权限控制的比较好，迁移也方便（推荐） 对6443端口发起请求 官方SDK 官方的GitHub有很多语言的支持，有Go、Java、Perl、Ruby； 栗子也很多也很简单，这里我就不费口舌了，自行Google。\nPython SDK地址：https://github.com/kubernetes-client/python\nPod内部调用 内部先获取Token，然后发起请求对apiserver\n1 2 TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token) curl --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; -s https://10.96.0.1:443/api/v1/namespaces/default/pods/ 参考链接：https://blog.csdn.net/russle/article/details/105333738\n6443 端口请求 Shell 版本：\n1 2 3 4 curl https://192.168.1.100:6443/api/v1/nodes \\ --cacert /etc/kubernetes/pki/ca.crt \\ --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt \\ --key /etc/kubernetes/pki/apiserver-kubelet-client.key Python 版本：\n1 2 3 4 5 6 7 8 url = \u0026#34;https://ip:6443/api/v1/nodes/unis6\u0026#34; try: res = requests.get(url, verify=\u0026#34;/etc/kubernetes/pki/ca.pem\u0026#34;, cert=(\u0026#34;/etc/kubernetes/pki/apiserver-kubelet-client.crt\u0026#34;,\u0026#34;/etc/kubernetes/pki/apiserver-kubelet-client.key\u0026#34;),timeout=15) except Exception as e: print(e) else: print(\u0026#34;ok\u0026#34;) metrics 接口一些调用方法： https://blog.csdn.net/u014106644/article/details/84839055\n","date":"2023-06-13T10:51:32+08:00","permalink":"https://caizhe.org/p/kubernetesapi%E8%B0%83%E7%94%A8%E6%96%B9%E6%B3%95/","title":"KubernetesApi调用方法"},{"content":"主节点：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 [root@59-139 ~]# docker run -d --net=host --cap-add NET_ADMIN \\ -e KEEPALIVED_AUTOCONF=true \\ # 角色 \\ -e KEEPALIVED_STATE=MASTER \\ # 绑定的网卡 \\ -e KEEPALIVED_INTERFACE=ens33 \\ # keep alive 通讯的标识ID \\ -e KEEPALIVED_VIRTUAL_ROUTER_ID=2 \\ # 广播地址 \\ -e KEEPALIVED_UNICAST_SRC_IP=192.168.59.139 \\ -e KEEPALIVED_UNICAST_PEER_0=192.168.59.140 \\ # 绑定的网卡 \\ -e KEEPALIVED_TRACK_INTERFACE_1=ens33 \\ # 虚拟VIP \\ -e KEEPALIVED_VIRTUAL_IPADDRESS_1=\u0026#34;192.168.59.100/24 dev ens33\u0026#34; \\ arcts/keepalived 从节点：\n1 2 3 4 5 6 7 8 9 10 [root@59-139 ~]# docker run -d --net=host --cap-add NET_ADMIN \\ -e KEEPALIVED_AUTOCONF=true \\ -e KEEPALIVED_STATE=BACKUP \\ -e KEEPALIVED_INTERFACE=ens33 \\ -e KEEPALIVED_VIRTUAL_ROUTER_ID=2 \\ -e KEEPALIVED_UNICAST_SRC_IP=192.168.59.140 \\ -e KEEPALIVED_UNICAST_PEER_0=192.168.59.139 \\ -e KEEPALIVED_TRACK_INTERFACE_1=ens33 \\ -e KEEPALIVED_VIRTUAL_IPADDRESS_1=\u0026#34;192.168.59.100/24 dev ens33\u0026#34; \\ arcts/keepalived 参考链接：https://hub.docker.com/r/arcts/keepalived\n","date":"2023-03-13T17:44:11+08:00","permalink":"https://caizhe.org/p/docker%E5%AE%89%E8%A3%85keepalived/","title":"Docker安装Keepalived"},{"content":" 名称 版本号 操作系统 Ubuntu 18 k8s 1.22 kubeVIP 0.5 nginx-ingress 1.3.1 关闭防火墙\nufw disable\r设置主机名\n1 hostnamectl set-hostname master1.innovsharing.com 设置hosts\n1 2 172.20.55.100 api.k8s.local 172.20.55.46 master1.innovsharing.com 开启内核IPV4转发\n1 2 3 4 cat /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 bridge-nf 使得 netfilter 可以对 Linux 网桥上的 IPv4/ARP/IPv6 包过滤。比如，设置net.bridge.bridge-nf-call-iptables＝1后，二层的网桥在转发包时也会被 iptables的 FORWARD 规则所过滤。常用的选项包括：\nnet.bridge.bridge-nf-call-arptables：是否在 arptables 的 FORWARD 中过滤网桥的 ARP 包\nnet.bridge.bridge-nf-call-ip6tables：是否在 ip6tables 链中过滤 IPv6 包\nnet.bridge.bridge-nf-call-iptables：是否在 iptables 链中过滤 IPv4 包\nnet.bridge.bridge-nf-filter-vlan-tagged：是否在 iptables/arptables 中过滤打了 vlan 标签的包。\n开启IPVS支持\n1 2 3 4 5 6 7 8 9 10 11 root@master1:~# cat /etc/modules-load.d/k8s.conf br_netfilter ip_vs ip_vs_rr ip_vs_wrr ip_vs_sh ## use nf_conntrack instead of nf_conntrack_ipv4 for Linux kernel 4.19 and later ## nf_conntrack_ipv4 nf_conntrack systemctl restart systemd-modules-load.service 重启服务器，然后执行lsmod | grep -e ip_vs -e nf_conntrack_ipv4,检查是否开启\n1 2 3 4 5 6 7 ip_vs_sh 16384 0 ip_vs_wrr 16384 0 ip_vs_rr 16384 0 ip_vs 155648 6 ip_vs_rr,ip_vs_sh,ip_vs_wrr nf_conntrack 139264 1 ip_vs nf_defrag_ipv6 24576 2 nf_conntrack,ip_vs libcrc32c 16384 2 nf_conntrack,ip_vs 安装IPVS\n1 apt-get install -y ipset ipvsadm 安装crontained\n1 2 3 4 5 6 7 8 wget https://github.com/containerd/containerd/releases/download/v1.6.8/cri-containerd-cni-1.6.8-linux-amd64.tar.gz # 或者国内下载： # wget https://download.fastgit.org/containerd/containerd/releases/download/v1.5.5/cri-containerd-cni-1.5.5-linux-amd64.tar.gz tar -C / -xzf cri-containerd-cni-1.6.8-linux-amd64.tar.gz mkdir -p /etc/containerd containerd config default \u0026gt; /etc/containerd/config.toml 配置crontained，文件位于/etc/containerd/config.toml\n1 2 3 4 5 6 7 8 9 10 11 12 [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc.options] SystemdCgroup = true [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.mirrors] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.mirrors.\u0026#34;docker.io\u0026#34;] endpoint = [\u0026#34;https://bqr1dr1n.mirror.aliyuncs.com\u0026#34;] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.mirrors.\u0026#34;k8s.gcr.io\u0026#34;] endpoint = [\u0026#34;https://registry.aliyuncs.com/google_containers\u0026#34;] sandbox_image = \u0026#34;registry.aliyuncs.com/k8sxio/pause:3.6\u0026#34; 启动crontained\n1 2 3 4 5 6 7 8 9 10 11 12 13 systemctl daemon-reload systemctl enable containerd --now root@master1:~# ctr version Client: Version: v1.6.8 Revision: 9cd3357b7fd7218e4aec3eae239db1f68a5a6ec6 Go version: go1.17.13 Server: Version: v1.6.8 Revision: 9cd3357b7fd7218e4aec3eae239db1f68a5a6ec6 UUID: 8804ad26-c5c3-4320-846a-b713c2307d5e 安装kube-VIP\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 mkdir -p /etc/kubernetes/manifests/ export VIP=172.20.55.100 export INTERFACE=eth0 ctr run --rm --net-host \\ docker.io/plndr/kube-vip:v0.5.0 \\ vip /kube-vip manifest pod \\ --interface $INTERFACE \\ --vip $VIP \\ --controlplane \\ --arp \\ --leaderElection | tee /etc/kubernetes/manifests/kube-vip.yaml \u0026gt; /etc/kubernetes/manifests/kube-vip.yaml kube-vip.yaml 内容\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 apiVersion: v1 kind: Pod metadata: creationTimestamp: null name: kube-vip namespace: kube-system spec: containers: - args: - manager env: - name: vip_arp value: \u0026#34;true\u0026#34; - name: port value: \u0026#34;6443\u0026#34; - name: vip_interface value: eth0 - name: vip_cidr value: \u0026#34;32\u0026#34; - name: cp_enable value: \u0026#34;true\u0026#34; - name: cp_namespace value: kube-system - name: vip_ddns value: \u0026#34;false\u0026#34; - name: vip_leaderelection value: \u0026#34;true\u0026#34; - name: vip_leaseduration value: \u0026#34;5\u0026#34; - name: vip_renewdeadline value: \u0026#34;3\u0026#34; - name: vip_retryperiod value: \u0026#34;1\u0026#34; - name: vip_address value: 172.20.55.100 - name: prometheus_server value: :2112 image: ghcr.io/kube-vip/kube-vip:v0.5.0 imagePullPolicy: Always name: kube-vip resources: {} securityContext: capabilities: add: - NET_ADMIN - NET_RAW volumeMounts: - mountPath: /etc/kubernetes/admin.conf name: kubeconfig hostAliases: - hostnames: - kubernetes ip: 127.0.0.1 hostNetwork: true volumes: - hostPath: path: /etc/kubernetes/admin.conf name: kubeconfig status: {} 安装kubeadm\n1 2 3 4 5 6 7 8 9 10 11 apt-get update \u0026amp;\u0026amp; apt-get install -y apt-transport-https curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - cat \u0026lt;\u0026lt;EOF \u0026gt;/etc/apt/sources.list.d/kubernetes.list deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main EOF apt-get update # apt-get install -y kubelet kubeadm kubectl apt-get install kubelet=1.22.9-00 kubeadm=1.22.9-00 kubectl=1.22.9-00 systemctl enable kubelet 生成K8S 安装配置清单文件\n1 kubeadm config print init-defaults --component-configs KubeletConfiguration \u0026gt; kubeadm.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 apiVersion: kubeadm.k8s.io/v1beta3 bootstrapTokens: - groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authentication kind: InitConfiguration localAPIEndpoint: advertiseAddress: 172.20.55.46 # 本机IP地址 bindPort: 6443 nodeRegistration: criSocket: /run/containerd/containerd.sock # 使用 containerd的Unix socket 地址 imagePullPolicy: IfNotPresent name: master1 # 节点名称 taints: null --- apiVersion: kubeproxy.config.k8s.io/v1alpha1 kind: KubeProxyConfiguration mode: ipvs # kube-proxy 模式 --- apiServer: timeoutForControlPlane: 4m0s apiVersion: kubeadm.k8s.io/v1beta3 certificatesDir: /etc/kubernetes/pki clusterName: kubernetes controllerManager: {} dns: {} etcd: local: dataDir: /var/lib/etcd imageRepository: registry.aliyuncs.com/google_containers # 阿里云镜像站 kind: ClusterConfiguration kubernetesVersion: 1.22.9 controlPlaneEndpoint: 172.20.55.100:6443 # 设置控制平面Endpoint地址 networking: dnsDomain: cluster.local serviceSubnet: 10.96.0.0/12 scheduler: {} --- apiVersion: kubelet.config.k8s.io/v1beta1 authentication: anonymous: enabled: false webhook: cacheTTL: 0s enabled: true x509: clientCAFile: /etc/kubernetes/pki/ca.crt authorization: mode: Webhook webhook: cacheAuthorizedTTL: 0s cacheUnauthorizedTTL: 0s cgroupDriver: systemd clusterDNS: - 10.96.0.10 clusterDomain: cluster.local cpuManagerReconcilePeriod: 0s evictionPressureTransitionPeriod: 0s fileCheckFrequency: 0s healthzBindAddress: 127.0.0.1 healthzPort: 10248 httpCheckFrequency: 0s imageMinimumGCAge: 0s kind: KubeletConfiguration logging: {} memorySwap: {} nodeStatusReportFrequency: 0s nodeStatusUpdateFrequency: 0s resolvConf: /run/systemd/resolve/resolv.conf rotateCertificates: true runtimeRequestTimeout: 0s shutdownGracePeriod: 0s shutdownGracePeriodCriticalPods: 0s staticPodPath: /etc/kubernetes/manifests streamingConnectionIdleTimeout: 0s syncFrequency: 0s volumeStatsAggPeriod: 0s root@master1:~# 下载镜像\n1 2 3 4 5 6 7 kubeadm config images pull --config kubeadm.yaml # core DNS 会报错，找不到，可以打一个tag ctr -n k8s.io i pull docker.io/coredns/coredns:1.8.4 ctr -n k8s.io i tag docker.io/coredns/coredns:1.8.4 registry.aliyuncs.com/k8sxio/coredns:v1.8.4 kubeadm init --upload-certs --config kubeadm.yaml Nginx-Ingress 下载地址：https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.3.1/deploy/static/provider/cloud/deploy.yaml\n让控制器使用主机网络（Deployment）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 ports: - containerPort: 80 name: http hostPort: 80 # 新增加 protocol: TCP - containerPort: 443 name: https hostPort: 443 # 新增加 protocol: TCP spec: hostNetwork: true # 全部使用主机网络 containers: 修改DNS策略：\n1 dnsPolicy: ClusterFirstWithHostNet 修改国内镜像：\nimage: registry.aliyuncs.com/google_containers/nginx-ingress-controller:v1.3.1\rimage: registry.aliyuncs.com/google_containers/kube-webhook-certgen:v1.3.1\rimage: registry.aliyuncs.com/google_containers/kube-webhook-certgen:v1.3.1\rTomcat测试：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 root@OPS:/opt# cat 02-tomcat.yaml apiVersion: v1 kind: Service metadata: name: tomcat namespace: default spec: selector: app: tomcat release: canary ports: - name: http targetPort: 8080 port: 8080 - name: ajp targetPort: 8009 port: 8009 --- apiVersion: apps/v1 kind: Deployment metadata: name: tomcat-deploy namespace: default spec: replicas: 2 selector: matchLabels: app: tomcat release: canary template: metadata: labels: app: tomcat release: canary spec: containers: - name: tomcat image: tomcat:8.5.34-jre8-alpine imagePullPolicy: IfNotPresent ports: - name: http containerPort: 8080 name: ajp containerPort: 8009 --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-myapp namespace: default annotations: kubernetes.io/ingress.class: \u0026#34;nginx\u0026#34; spec: rules: - host: www.example.com http: paths: - path: / pathType: Prefix backend: service: name: tomcat port: number: 8080 普通的 Service： 会生成servicename.namespace.svc.cluster.local的域名，会解析到 Service 对应的 ClusterIP 上，在 Pod 之间的调用可以简写成servicename.namespace， 如果处于同一个命名空间下面​​，甚至可以只写成 ​​servicename​​ 即可访问\nHeadless Service： 无头服务，就是把 clusterIP 设置为 None 的，会被解析为指定 Pod 的 IP 列表，同样还可以通过podname.servicename.namespace.svc.cluster.local访问到具体的某一个 Pod。\n","date":"2023-03-13T11:51:59+08:00","permalink":"https://caizhe.org/p/kubenertes1.22%E9%AB%98%E5%8F%AF%E7%94%A8%E5%AE%89%E8%A3%85/","title":"Kubenertes1.22高可用安装"},{"content":"镜像 拉取镜像\nctr image pull docker.io/library/nginx:latest\r查看镜像\nctr image ls -q\rTAG标签\nctr image tag docker.io/library/nginx:latest caizhe.org/library/nginx:latest\r删除镜像\nctr images rm caizhe.org/library/nginx:latest\r挂载镜像到本地目录\nctr image mount docker.io/library/nginx:latest /mnt\r取消挂载\nctr image unmount /mnt\r导出\nctr image export nginx.tar.gz docker.io/library/nginx:latest\r导入\nctr i import nginx.tar.gz\r解决ctr: content digest sha256:xxxxxx not found的错误\n1 2 3 4 5 6 7 8 方法1： ctr image pull --platform amd64 docker.io/library/nginx:latest ctr i export --platform amd64 nginx.tar.gz docker.io/library/nginx:latest ctr i import --platform amd64 nginx.tar.gz 方法2： ctr i pull --all-platforms docker.io/library/nginx:latest ctr i export --all-platforms nginx.tar.gz docker.io/library/nginx:latest ctr i import nginx.tar.gz 容器 创建容器\nctr container create docker.io/library/nginx:latest nginx\r查看容器\nctr container ls\r删除容器\nctr container rm nginx\r启动容器\nctr task start -d nginx\r直接启动容器（创建容器+启动容器）\nctr run -d --net-host nginx bash\r查看容器状态\nctr task ls\r进入容器\nctr task exec -t --exec-id 0 nginx /bin/bash\r停止容器\nctr task kill nginx\r暂停容器\nctr task pause nginx\r查看容器资源使用情况\nctr task metrics nginx\r名称空间 查看名称空间\nctr ns ls\r创建名称空间\nctr ns create test\r删除名称空间\nctr ns rm test\r提示：docker的名称空间是moby，k8s的名称空间是k8s.io\n","date":"2023-03-02T17:44:11+08:00","permalink":"https://caizhe.org/p/containerd%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","title":"Containerd常用命令"},{"content":"1、下载依赖安装包\n1 2 3 wget https://github.com/containerd/containerd/releases/download/v1.6.4/cri-containerd-cni-1.6.4-linux-amd64.tar.gz # 如果无法访问github可以使用下面地址 # wget https://download.fastgit.org/containerd/containerd/releases/download/v1.6.4/cri-containerd-cni-1.6.4-linux-amd64.tar.gz 2、解压\n1 tar -C / -xzf cri-containerd-cni-1.6.4-linux-amd64.tar.gz 3、添加配置文件：\n默认配置文件为 /etc/containerd/config.toml，我们可以通过如下所示的命令生成\n1 2 mkdir -p /etc/containerd containerd config default \u0026gt; /etc/containerd/config.toml 4、启动containerd\n1 systemctl enable containerd --now 5、验证containerd\n1 2 3 4 5 6 7 8 9 10 # ctr version Client: Version: v1.6.4 Revision: 72cec4be58a9eb6b2910f5d10f1c01ca47d231c0 Go version: go1.16.6 Server: Version: v1.6.4 Revision: 72cec4be58a9eb6b2910f5d10f1c01ca47d231c0 UUID: 38613830-5cd0-4bc4-81b4-2bcdced721d3 ","date":"2023-03-01T17:44:11+08:00","permalink":"https://caizhe.org/p/containerd%E5%AE%89%E8%A3%85/","title":"Containerd安装"},{"content":"1、首先需要关闭远程仓库的分支保护，允许强制推送：\n\u0026ldquo;Settings\u0026rdquo; -\u0026gt; \u0026ldquo;Repository\u0026rdquo; -\u0026gt; scroll down to \u0026ldquo;Protected branches\u0026rdquo;.\n2、克隆项目,拉取所有分支：\n1 2 3 4 5 git clone xxx # 默认拉取master分支 cd xx #进入拉取的文件夹 git branch -r | grep -v \u0026#39;\\-\u0026gt;\u0026#39; | while read remote; do git branch --track \u0026#34;${remote#origin/}\u0026#34; \u0026#34;$remote\u0026#34;; done git fetch --all git pull --all 3、查找大文件（将最大的10个文件查询下来）\n1 git rev-list --objects --all | grep \u0026#34;$(git verify-pack -v .git/objects/pack/*.idx | sort -k 3 -n | tail -10 | awk \u0026#39;{print$1}\u0026#39;)\u0026#34; 4、清理查找的大文件（一次只能处理一个文件/文件夹）：\n1 git filter-branch --force --index-filter \u0026#39;git rm -rf --cached --ignore-unmatch 目录/文件\u0026#39; --prune-empty --tag-name-filter cat -- --all 5、删除并回收空间\n1 2 3 4 5 git for-each-ref --format=\u0026#39;delete %(refname)\u0026#39; refs/original | git update-ref --stdin rm -rf .git/refs/original/ git reflog expire --expire=now --all git gc --prune=now git gc --aggressive --prune=now 6、推送远程仓库：\n1 2 git push origin --force --all git remote prune origin 然后其他人重新克隆项目即可\n参考链接：https://www.msnao.com/2021/06/15/5031.html\n","date":"2023-03-01T17:44:11+08:00","permalink":"https://caizhe.org/p/gitlab%E5%A4%A7%E6%96%87%E4%BB%B6%E6%B8%85%E7%90%86/","title":"Gitlab大文件清理"},{"content":" 在全民HTTPS的大趋势之下，Let\u0026rsquo;s Encrypt可是功不可没，网上文档也有很多，本身很简单的东西，网上文档比17年初已经多了很多很多，但是没想到水平参差不齐，部署到一半就不行了，简直是坑爹，就16年的老技术了，简单的几个步骤还能报错？！！！\n官方GitHub地址：https://github.com/Neilpang/acme.sh\n安装acme.sh脚本\n1 2 3 4 5 6 7 curl https://get.acme.sh | sh # 注册账户 ~/.acme.sh/acme.sh --register-account -m bob1317581669@gmail.com [Wed Feb 22 11:01:51 CST 2023] No EAB credentials found for ZeroSSL, let\u0026#39;s get one [Wed Feb 22 11:01:55 CST 2023] Registering account: https://acme.zerossl.com/v2/DV90 [Wed Feb 22 11:02:02 CST 2023] Registered [Wed Feb 22 11:02:03 CST 2023] ACCOUNT_THUMBPRINT=\u0026#39;yiMmaNSo-A27AG-jSqTkWrKKs7PgL7X9rPXApgxxxxx\u0026#39; 单域名证书 验证域名所有权\nacme.sh --issue -d cmdb.caizhe.org --nginx\r安装证书\n（其实证书已经下来了，就在acme的目录下面，但是你不想放在这里话，可以执行这一步）\n1 2 3 4 [root@Bob-blog conf]# acme.sh --installcert -d cmdb.caizhe.org \\ --keypath /application/nginx/conf/cmdb_ssl/cmdb.caizhe.org.key \\ --fullchainpath /application/nginx/conf/cmdb_ssl/cmdb.caizhe.org.cer \\ --reloadcmd \u0026#34;/application/nginx/sbin/nginx -s reload\u0026#34; 查看域名情况\n1 2 3 [root@Bob-blog conf]# acme.sh --list Main_Domain KeyLength SAN_Domains Created Renew cmdb.caizhe.org \u0026#34;\u0026#34; no Wed Jun 5 09:19:13 UTC 2019 Sun Aug 4 09:19:13 UTC 2019 域名更新(之后跑个定时就行了)\nacme.sh --renew -d cmdb.caizhe.org --force\r通配符证书 1 2 3 4 export Ali_Key=\u0026#34;LTAI4FyNQRVTRaGc1xxxxx\u0026#34; export Ali_Secret=\u0026#34;cE5Nhz8lRgCZ2lLxLP3ADjhIxxxxx\u0026#34; ~/.acme.sh/acme.sh --issue --dns dns_ali -d \u0026#39;caizhe.org\u0026#39; -d \u0026#39;*.caizhe.org\u0026#39; 通配符的https必须是用DNS模式，目前支持阿里DNS和DNSpod；\n如果你是用的DNS 有支持的API接口，它会自己做验证和更新，\n关于这个Let\u0026rsquo;s Encrypt 我现在已经全站使用了近两年的时间了，没什么问题，大家可以放心使用。\ncertbot 更新通配符\ncertbot-auto --server https://acme-v02.api.letsencrypt.org/directory -d \u0026quot;*.xxx.com\u0026quot; --manual --preferred-challenges dns-01 certonly\r附录Nginx配置\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 server { listen 80; server_name cmdb.caizhe.org; location /.well-known { alias /application/nginx/html/cmdb/.well-known; } rewrite ^(.*) https://$server_name$1 last; error_page 497 https://$server_name$request_uri; } server { listen 443 ssl; server_name cmdb.caizhe.org; ssl_certificate /root/.acme.sh/cmdb.caizhe.org/cmdb.caizhe.org.cer; ssl_certificate_key /root/.acme.sh/cmdb.caizhe.org/cmdb.caizhe.org.key; ssl_ciphers EECDH+CHACHA20:EECDH+CHACHA20-draft:EECDH+AES128:RSA+AES128:EECDH+AES256:RSA+AES256::!MD5; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on; #ssl_session_cache builtin:1000 shared:SSL:10m; #ssl_session_timeout 1d; #ssl_session_tickets on; location / { root html/cmdb; index index.html index.htm; } } ","date":"2017-06-13T10:59:21+08:00","permalink":"https://caizhe.org/p/letsencrypt/","title":"LetsEncrypt"},{"content":"Redis 持久化方式 Redis 提供了多种不同级别的持久化方式：\nRDB 持久化可以在指定的时间间隔内生成数据集的时间点快照（point-in-time snapshot）。\rAOF AOF 持久化记录服务器执行的所有写操作命令，并在服务器启动时，通过重新执行这些命令来还原数据集。\tRedis 还可以同时使用 AOF 持久化和 RDB 持久化。 在这种情况下， 当 Redis 重启时， 它会优先使用 AOF 文件来还原数据集， 因为 AOF 文件保存的数据集通常比 RDB 文件所保存的数据集更完整。\nRDB 优缺点 优：故障恢复的时候特别快，最大化Redis性能；直接fork一个子进程，之后的所有保存工作全交给子进程；支持的功能更多。\n缺：如果在未保存快照的时候故障，会丢失部分数据；子进程在fork的时候会非常影响性能；可能会造成客户端连接的停顿。\nAOF 优缺点 优：数据相比RDB可能更完整一些，因为是每秒写入一次，最多丢一秒的数据；操作追加到日志文件，可重写； 持久化记录服务器执行的所有写操作命令，并在服务器启动时，通过重新执行这些命令来还原数据集。新命令会被追加到文件的末尾。 Redis 还可以在后台对 AOF 文件进行重写（rewrite），使得 AOF 文件的体积不会超出保存数据集状态所需的实际大小。 （什么是重写：set a 1，set a 2 set a 3，只记录set a 3）\n缺：体积大于RDB，恢复速度慢于RDB\nRrdis 主从原理 1、当从库和主库建立MS关系后，会向主数据库发送PSYNC命令；\r2、主库接收到PSYNC命令后会开始在后台保存快照（RDB持久化过程），并将期间接收到的写请求写到缓存区；\r3、当快照完成后，主Redis会将快照文件和所有缓存的写命令发送给从Redis；\r4、从Redis接收到后，会载入快照文件并且执行收到的缓存的命令；\r5、主Redis每当接收到写命令时就会将命令发送从Redis，从而保证数据的一致；\rRedis 优化 更改端口\nport 6379\t设置密码\nrequirepass XXXXXX\r绑定地址\nbind 10.0.0.10\r后台运行\ndaemonize yes\r日志级别\nloglevel warning\r日志位置\nlogfile \u0026quot;/var/log/redis_6379.log\u0026quot;\r持久化\n建议master关闭，slave开启\nsave 900 1\rsave 300 10\rsave 60 10000 开启只读\nslave-read-only yes\r慢查询\nslowlog-log-slower-than 5000\r最大内存\nmaxmemory 5gb\r数据淘汰机制 volatile-lru：使用LRU算法进行数据淘汰（淘汰上次使用时间最早的，且使用次数最少的key），只淘汰设定了有效期的key\rallkeys-lru：使用LRU算法进行数据淘汰，所有的key都可以被淘汰\rvolatile-random：随机淘汰数据，只淘汰设定了有效期的key\rallkeys-random：随机淘汰数据，所有的key都可以被淘汰\rvolatile-ttl：淘汰剩余有效期最短的key\r","date":"2017-03-02T20:52:54+08:00","permalink":"https://caizhe.org/p/redis%E4%BC%98%E5%8C%96/","title":"Redis优化"},{"content":"系统优化 1.精简开机自启动\n2.禁止ROOT远程登录，更改SSH端口，远程登录改为秘钥认证，如果有必要换成VPN内网连接。\n3.根据业务关闭IPtables、关闭Selinux。\n4.更换国内较快的YUM源\n5.调整文件字符集，最好为UTF-8\n7.修改系统登录信息 /etc/issue\n8.时间同步\n9.设置连接终端超时\n10.不要使用IP地址，尽量使用主机名。\n11.锁文件（慎用）\n6.调整文件描述符大小\nvim /etc/security/limits.conf\r* soft nofile 65535\r* hard nofile 65535 内核优化 开启内核转发\nnet.ipv4.ip_forward = 1\r设置timewait的数量，默认180000\nnet.ipve.tcp_max_tw_buckets = 6000\r用来设定允许系统打开的端口范围。\nnet.ipv4.ip_local_port_range = 1024 65000\r设置启用timewait快速回收。\nnet.ipv4.tcp_tw_recycle = 1\r用于设置开启重用，允许将TIME-WAIT sockets重新用于新的TCP连接\nnet.ipv4.tcp_tw_reuse = 1\r开启SYN Cookies，当出现SYN等待队列溢出时，启用cookies进行处理。\nnet.ipv4.tcp_syncookies = 1\r用于调节系统同时发起的tcp连接数，在高并发的请求中，默认的值可能会导致链接超时或者重传，因此，需要结合并发请求数来调节此值。默认值是128\nnet.core.somaxconn = 262144\r当每个网络接口接收数据包的速率比内核处理这些包的速率快时，允许发送到队列的数据包的最大数目。\nnet.core.netdev_max_backlog = 262144\r用于设定系统中最多有多少个TCP套接字不被关联到任何一个用户文件句柄上。如果超过这个数字，孤立连接将立即被复位并打印出警告信息。这个限制只是为了防止简单的DoS攻击。不能过分依靠这个限制甚至人为减小这个值，更多的情况下应该增加这个值\nnet.ipv4.tcp_max_orphans = 262144\r记录那些尚未收到客户端确认信息的连接请求的最大值。对于有128MB内存的系统而言，此参数的默认值是1024，对小内存的系统则是128\nnet.ipv4.tcp_max_syn_backlog = 262144\r内核放弃连接之前发送SYN+ACK包的数量。\nnet.ipv4.tcp_synack_retries = 1\r在内核放弃建立连接之前发送SYN包的数量。\nnet.ipv4.tcp_syn_retries = 1\r设置套接字保持在FIN-WAIT-2状态的时间。默认值是60秒\nnet.ipv4.tcp_fin_timeout = 1\r当长连接启用的时候，TCP发送长连接消息的频度。默认值是2小时\nnet.ipv4.tcp_keepalive_time = 30\r调整使用swap分区的使用率/百分百。(100-10)90，当内存使用率达到90使用swap，redis服务器建议为0\n/proc/sys/vm/swappiness 10\r这篇文章较短，未来会不断完善，欢迎大家提供建议。\n","date":"2016-03-02T20:49:41+08:00","permalink":"https://caizhe.org/p/linux%E7%B3%BB%E7%BB%9F%E4%BC%98%E5%8C%96/","title":"linux系统优化"},{"content":"nginx 的优化分为两个小方向：性能优化 \u0026amp; 安全优化\n性能优化 开启gzip压缩\ngzip on;\rgzip_min_length 1k;\rgzip_buffers 4 32k;\rgzip_http_version 1.1;\rgzip_comp_level 2;\rgzip_types text/css text/js text/xml application/javascript;\rgzip_vary on;\r使用expires 缓存\nlocation ~^/(images|javascript|js|css|flash|media|static)/ {\rexpires 30d;\r}\rlocation ~ .*\\.(gif|jpg|jpeg|png|bmp|swf)$\r{\rexpires 3650d;\r}\r使用epoll模型\nuse epoll;\r配置多个work进程\n与CPU的核数相等 or CPU的核数 X2\r调整当进程的最大连接数\nworker_connections 20480；\r调整进程可打开的的最大文件数\nworker_rlimit_nofile 65535;\r设置连接超时\nkeepalive_timeout 300;\rJAVA应用经量使用长连接、如果是PHP的尽量使用短连接，因为Java应用要消耗的资源和时间要比PHP跟多（具体依然要看业务）\r客户端请求头超时时间/大小\nclient_header_timeout 15s;\tclient_header_buffer_size 10k;\r客户端请求主体超时时间\nclient_header_timeout 15s;\r用户上传大小的限制\nclient_max_body_size 10m;\r开启高效文件传输模式\nsendfile on;\rtcp_nopush on;\raio on;\r为后端代理服务器设置缓冲区\nproxy_buffering on;\rproxy_buffer_size 64k;\rproxy_buffers 4 32k;\rproxy_busy_buffers_size 64k;\rproxy_temp_file_write_size 64k;\r制定客户端的超时时间\nsend_timeout 300s;\r安全优化 隐藏版本号\nserver_tokens off;\r更改默认用户组\nuser nginx;\r静止解析制定目录下的特定文件\nlocation ~ ^/images/.*\\.(php|php5|.sh|.pl|.py)$ { deny all; } 限制及指定IP或IP段访问\nlocation / { deny 192.168.1.1; allow 192.168.1.0/24; deny all; }\r限制非法IP\nif ($remote_addr = 10.0.0.2 ) {\rreturn 403;\r}\r禁止非法域名解析访问\nif （$host !~ caizhe/.org$）{\rreturn 500;\r}\rserver {\rlisten 80 default;\rreturn 500;\r}\r限制访问频率\nhttp {\tlimit_req_zone $binary_remote_addr zone=ttlsa_com:10m rate=1r/s;\rserver {\rlimit_req zone=one burst=5;\r错误页面优雅显示\nerror_page 500 501 502 503 504 https://caizhe.org;\rerror_page 400 403 404 405 408 410 411 412 413 414 415 https://caizhe.org;\r使用普通用户启动nginx\n","date":"2016-03-02T20:46:18+08:00","permalink":"https://caizhe.org/p/nginx%E4%BC%98%E5%8C%96/","title":"Nginx优化"}]